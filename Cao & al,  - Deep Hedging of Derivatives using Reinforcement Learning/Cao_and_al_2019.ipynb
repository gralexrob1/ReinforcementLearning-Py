{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5581390b",
      "metadata": {
        "id": "5581390b"
      },
      "source": [
        "# Cao, Chen, Hull, Poulos, 2019 - Deep Hedging of Derivatives Using Reinforcement Learning\n",
        "\n",
        "This notebook proposes an implementation of the concepts described in the article mentionned above.\n",
        "\n",
        "**For now, only the parameters to to try and reproduce Table 1 are used**\n",
        "\n",
        "*Option parameters*\n",
        "- Short call option\n",
        "- Maturity: 1 month\n",
        "- Strike: At the money\n",
        "- Trading cost: 1%\n",
        "\n",
        "*Underlying parameters*\n",
        "- Geometric Brownian motion\n",
        "- Volatility: 20%\n",
        "- Expected return: 5%\n",
        "- Dividend yield: 0%\n",
        "- Risk-free rate: 0%\n",
        "\n",
        "\n",
        "\n",
        "**Objective function**\n",
        "\n",
        "$$ Y(t) = \\mathbb{E}[C_t] + c \\cdot \\sqrt{ \\mathbb{E}[C_t^2] - \\mathbb{E}[C_t]^2 }$$\n",
        "where:\n",
        "- $C_t$ is the total hedging cost from time $t$ onward\n",
        "- $\\mathbb{E}[C_t]$ is the expected hedging cost\n",
        "- $c$ is a constant\n",
        "\n",
        "The objective is to minimize $Y(0)$.\n",
        "Y is a coherent risk measure and satisfies Bellman equation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "0rASVR9kc8qC"
      },
      "id": "0rASVR9kc8qC"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2966d9dc",
      "metadata": {
        "id": "2966d9dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1e24ca-8cf9-479c-b50c-6694954cace7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.22.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "\n",
        "import datetime as dt\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces # https://gymnasium.farama.org/api/spaces/\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy.random as sim\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(2023)\n",
        "np.random.seed(2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Files"
      ],
      "metadata": {
        "id": "7FryOZtOAobr"
      },
      "id": "7FryOZtOAobr"
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "\n",
        "# WORK_DRIVE = '/gdrive'\n",
        "# WORK_AREA = WORK_DRIVE + '/MyDrive/Cao and al, 2019 - Deep Hedging of Derivatives using Reinforcement Learning'\n",
        "# drive.mount(WORK_DRIVE)\n",
        "# os.chdir(WORK_AREA)\n",
        "\n",
        "# if not os.path.isdir('Cao and al, 2019 - models'):\n",
        "#     os.makedirs('Cao and al, 2019 - models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JT5mELim45o",
        "outputId": "0911ac6e-2c31-459a-9229-7bc95c2ff44f"
      },
      "id": "0JT5mELim45o",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n",
        "\n",
        "In the following we will use the below convention:\n",
        "- Number of days in a year: 252\n",
        "- Number of days in a month: 21"
      ],
      "metadata": {
        "id": "yXDUBn3xdUcA"
      },
      "id": "yXDUBn3xdUcA"
    },
    {
      "cell_type": "markdown",
      "id": "c74eb41f",
      "metadata": {
        "id": "c74eb41f"
      },
      "source": [
        "## Geometric brownian motion\n",
        "$$ dS = \\mu S dt + \\sigma S dBt $$\n",
        "\n",
        "*Euler scheme*\n",
        "$$ S_{t+1} = S_t + \\mu S_t \\Delta t + S_t \\sigma \\Delta B_t $$\n",
        "where:\n",
        "- $\\Delta B_t = \\sqrt{\\Delta t} \\cdot G$\n",
        "- $ G \\sim \\mathcal{N}(0,1)$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_geometric_brownian_motion(S0, mu, sigma, n, N, dt=1/252, plot=False):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    S0: float\n",
        "        Initial underlying price\n",
        "    mu: float\n",
        "        Expected return\n",
        "    sigma: float\n",
        "        Volatility\n",
        "    n: int\n",
        "        Number of increments (in days, convention is 252 per year)\n",
        "    N: int\n",
        "        Number of trajectory\n",
        "    dt: float\n",
        "        Size of increment (convention is 1/252)\n",
        "    \"\"\"\n",
        "\n",
        "    S = np.ones((n+1, N)) * S0\n",
        "\n",
        "    for j in range(N) :\n",
        "        for i in range(1, n+1) :\n",
        "            S[i,j] = S[i-1,j] * (1 + mu*dt + sigma*np.sqrt(dt)*sim.randn())\n",
        "\n",
        "    if plot:\n",
        "        plot_process(S)\n",
        "        plt.show()\n",
        "\n",
        "    return S\n",
        "\n",
        "def plot_process(S):\n",
        "    n, _ = S.shape\n",
        "    dates = np.linspace(0, n-1, n)\n",
        "\n",
        "    plt.plot(dates, S)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XVc8BFHU_4fY"
      },
      "id": "XVc8BFHU_4fY",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "PSB-eh2M_6Is"
      },
      "id": "PSB-eh2M_6Is"
    },
    {
      "cell_type": "code",
      "source": [
        "# simulate_geometric_brownian_motion(\n",
        "#     S0 = 100,\n",
        "#     mu = 5/100,\n",
        "#     sigma = 20/100,\n",
        "#     n = 252, # 1 year\n",
        "#     N = 100,\n",
        "#     plot=True\n",
        "# )"
      ],
      "metadata": {
        "id": "H3GOjStO4bqf"
      },
      "id": "H3GOjStO4bqf",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## European call option framework\n",
        "> Black and Scholes (1973)\n",
        "> Merton (1973)\n",
        "\n",
        "$$ S_0 e^{-qT} \\mathcal{N}(d_1) - K e^{-rT} \\mathcal{N}(d_2)$$\n",
        "where:\n",
        "- $r$ is the risk free rate\n",
        "- $q$ is the dividend yield\n",
        "- $S_0$ the initial value of $S$\n",
        "- $d_1 = \\frac{ ln(S_0/K) + (r-q+\\sigma^2/2)T}{\\sigma \\sqrt{T}}$\n",
        "- $d_2 = d_1 - \\sigma \\sqrt{T}$\n"
      ],
      "metadata": {
        "id": "KTzrgONsy1V6"
      },
      "id": "KTzrgONsy1V6"
    },
    {
      "cell_type": "code",
      "source": [
        "def black_scholes_d1(S, K, r, q, sigma, T):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  S: float\n",
        "      Underlying price\n",
        "  K: float\n",
        "      Strike price\n",
        "  r: float\n",
        "      Risk-free rate\n",
        "  q: float\n",
        "      Dividend yield\n",
        "  sigma: float\n",
        "      Volatility\n",
        "  T: int\n",
        "      Residual maturity (in years)\n",
        "  \"\"\"\n",
        "  return (np.log(S/K) + T*(r - q + 0.5*sigma**2) ) / (sigma * np.sqrt(T))\n",
        "\n",
        "def black_scholes_call(S, K, r, q, sigma, T):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  S: float\n",
        "      Underlying price\n",
        "  K: float\n",
        "      Strike price\n",
        "  r: float\n",
        "      Risk-free rate\n",
        "  q: float\n",
        "      Dividend yield\n",
        "  sigma: float\n",
        "      Volatility\n",
        "  T: int\n",
        "      Residual maturity\n",
        "  \"\"\"\n",
        "  d1 = black_scholes_d1(S, K, r, q, sigma, T)\n",
        "  d2 = d1 - sigma * np.sqrt(T)\n",
        "  return S * np.exp(-q*T) * stats.norm.cdf(d1) - K * np.exp(-r*T) * stats.norm.cdf(d2)\n",
        "\n",
        "def black_scholes_delta(S, K, r, q, sigma, T):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  S: float\n",
        "      Underlying price\n",
        "  K: float\n",
        "      Strike price\n",
        "  r: float\n",
        "      Risk-free rate\n",
        "  q: float\n",
        "      Dividend yield\n",
        "  sigma: float\n",
        "      Volatility\n",
        "  T: int\n",
        "      Residual maturity\n",
        "  \"\"\"\n",
        "  d1 = black_scholes_d1(S, K, r, q, sigma, T)\n",
        "  return np.exp(-q*T) * stats.norm.cdf(d1)"
      ],
      "metadata": {
        "id": "UfjD99XNywKq"
      },
      "id": "UfjD99XNywKq",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "ExGVgQpC_-ME"
      },
      "id": "ExGVgQpC_-ME"
    },
    {
      "cell_type": "code",
      "source": [
        "# print(black_scholes_call(100, 100, 2/100, 0, 20/100, 1) * 10000) # approx 90000 as per Cao & al (2019)\n",
        "# print('\\tvs 90000')\n",
        "# print(black_scholes_delta(100, 100, 2/100, 0, 20/100, 1) * 10000) # approx 5800 as per Cao & al (2019)\n",
        "# print('\\tvs5800')\n",
        "\n",
        "# print(black_scholes_delta(115, 100, 2/100, 0, 20/100, 6/12)) # approx 0.87 as per Cao & al (2019)\n",
        "# print('\\tvs 0.87')\n",
        "\n",
        "# print(black_scholes_delta(115*(1.1), 100, 2/100, 0, 20/100, 5/12)) # approx 0.94 as per Cao & al (2019)\n",
        "# print('\\tvs 0.94')\n",
        "# print(black_scholes_delta(115*(0.9), 100, 2/100, 0, 20/100, 5/12)) # approx 0.65 as per Cao & al (2019)\n",
        "# print('\\tvs 0.65')"
      ],
      "metadata": {
        "id": "F4YSSOafe6H5"
      },
      "id": "F4YSSOafe6H5",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Financial data"
      ],
      "metadata": {
        "id": "GHulOqcAC_P7"
      },
      "id": "GHulOqcAC_P7"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_financial_data(underlying_params, option_params, trading_params):\n",
        "    \"\"\"Simulation of financial data as per modeling of the situation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    underlying_params: tuple(float, float, float, float, int, int)\n",
        "        Parameters for geometric Brownian simulation\n",
        "        (initial underlying price, expected return, volatility, number of increment, number of trajectory)\n",
        "        Here the number of increment corresponds to the year base (ex: 252 days per year)\n",
        "    option_params:\n",
        "        Parameters for Black and Scholes european option evaluation\n",
        "        (spot price, strike, risk-free rate, dividend yield, volatility, residual maturity)\n",
        "    trading_params: tuple(float, float)\n",
        "        Trading parameters corresponding\n",
        "        (trade frequency per day, trade cost)\n",
        "    \"\"\"\n",
        "\n",
        "    S0, mu, sigma, n, N = underlying_params\n",
        "    S, K, r, q, sigma, T = option_params\n",
        "    trading_freq, trading_cost = trading_params\n",
        "\n",
        "    # Cf 'Modeling of the situation' section (we only model step with rebalancing)\n",
        "    rebalancing_n = int(T/trading_freq)\n",
        "    rebalancing_interval = 1/n*trading_freq\n",
        "    # We have indeed: rebalancing_n * rebalancing = T/n = option_life in annual base n\n",
        "\n",
        "    # Spot prices\n",
        "    S = simulate_geometric_brownian_motion(S0, mu, sigma, n=rebalancing_n, N=N, dt=rebalancing_interval)\n",
        "    underlying_prices = S.T\n",
        "\n",
        "    # Residual maturities, option_prices, option_deltas\n",
        "    M_res = np.arange(T, -trading_freq, -trading_freq)\n",
        "    res_maturities = M_res[M_res >= 0]\n",
        "    option_prices = black_scholes_call(underlying_prices, K, r, q, sigma, res_maturities/n)\n",
        "    option_deltas = black_scholes_delta(underlying_prices, K, r, q, sigma, res_maturities/n)\n",
        "\n",
        "    return underlying_prices, option_prices, option_deltas, res_maturities"
      ],
      "metadata": {
        "id": "w2Mf4jnnDGQ9"
      },
      "id": "w2Mf4jnnDGQ9",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "slwRbqW3DGYr"
      },
      "id": "slwRbqW3DGYr"
    },
    {
      "cell_type": "code",
      "source": [
        "# S0 = 100\n",
        "# mu = 5/100\n",
        "# sigma = 20/100\n",
        "# n = 252 # will correspond to the year base in days in the function\n",
        "# N = 100\n",
        "# underlying_params = (S0, mu, sigma, n, N)\n",
        "\n",
        "# S = 100\n",
        "# K = 100\n",
        "# r = 0\n",
        "# q = 0\n",
        "# T = 21\n",
        "# option_params = (S, K, r, q, sigma, T)\n",
        "\n",
        "# trading_freq = 4\n",
        "# trading_cost = 1/100\n",
        "# trading_params = (trading_freq, trading_cost)\n",
        "\n",
        "\n",
        "# underlying_prices, option_prices, option_deltas, res_maturities = get_financial_data(underlying_params, option_params, trading_params)\n",
        "# print(underlying_prices.shape)\n",
        "# print(option_prices.shape)\n",
        "# print(option_deltas.shape)\n",
        "\n",
        "# # Plot\n",
        "# fig, axes = plt.subplots(1, 3, figsize=(10,4))\n",
        "\n",
        "# axes[0].plot(underlying_prices.T)\n",
        "# axes[0].set_title('Underlying Prices')\n",
        "# axes[0].set_xlabel('Days')\n",
        "# axes[0].set_ylabel('Underlying Price')\n",
        "\n",
        "# axes[1].plot(option_prices.T)\n",
        "# axes[1].set_title('Option Prices')\n",
        "# axes[1].set_xlabel('Days')\n",
        "# axes[1].set_ylabel('Option Price')\n",
        "\n",
        "# axes[2].plot(option_deltas.T)\n",
        "# axes[2].set_title('Option Deltas')\n",
        "# axes[2].set_xlabel('Days')\n",
        "# axes[2].set_ylabel('Option Delta')\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(res_maturities.reshape(len(res_maturities),1))\n",
        "# plt.xlabel('Steps')\n",
        "# plt.ylabel('Days')\n",
        "# plt.title('Residual Maturity')"
      ],
      "metadata": {
        "id": "jS0XafNlDGgB"
      },
      "id": "jS0XafNlDGgB",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "QO7hwbmHDY17"
      },
      "id": "QO7hwbmHDY17"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We suppose rebalancing of the position is done at time intervals $\\Delta t$.\n",
        "Rebalancing of the position is subject to trading costs $\\kappa$.\n",
        "The life of the option is $n \\cdot \\Delta t$."
      ],
      "metadata": {
        "id": "Cw0GGD0iJY1E"
      },
      "id": "Cw0GGD0iJY1E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**State**\n",
        "At time $i \\cdot \\Delta t$.\n",
        "Defined by 3 parameters:\n",
        "\n",
        "- Asset price at time $i \\Delta t$\n",
        "\n",
        "- Holding of the asset from time $(i-1) \\cdot \\Delta t$ to time $i \\cdot \\Delta t$\n",
        "\n",
        "- Time to maturity\n",
        "\n",
        "\n",
        "\n",
        "**Action**\n",
        "At time $i \\cdot \\Delta t$.\n",
        "Amount of asset to be held from $i \\cdot \\Delta t$ to $(i+1) \\cdot \\Delta t$.\n",
        "\n",
        "\n",
        "\n",
        "**Reward - Accounting P&L Formulation**\n",
        "Reward corresponds to .\n",
        "\n",
        "$$ R_0 = V_0 -\\kappa \\cdot |S_0 \\cdot H_0| $$\n",
        "$$ R_{i+1} = V_{i+1} - V_i + H_i \\cdot (S_{i+1}-S_i) - \\kappa \\cdot |{S_{i+1} \\cdot (H_{i+1}-H_i)}| $$\n",
        "$$ R_n = V_{n} - V_{n-1} + H_{n-1} \\cdot (S_{n}-S_{n-1}) -\\kappa \\cdot |S_{n} \\cdot H_{n}| $$\n",
        "for $0 \\le i \\le n$\n",
        "where:\n",
        "- $S_i$ is the asset price at the beginning of the period\n",
        "- $H_i$ is the holding between time $i \\Delta t$ and $(i+1) \\Delta t$\n",
        "- $\\kappa$ is the trading cost as a proportion of value\n",
        "- $V_i$ is the value of the derivative position at the beginning of period $i$\n",
        "- $V_n$ is the derivative position payoff\n",
        "\n",
        "Note that $R_0$ is negative (short call position value + hedge set up).\n"
      ],
      "metadata": {
        "id": "YeZzfJXGH1vs"
      },
      "id": "YeZzfJXGH1vs"
    },
    {
      "cell_type": "code",
      "source": [
        "class HedgingEnv(gym.Env): # https://gymnasium.farama.org/api/env/\n",
        "    \"\"\"An environment for heding simulations\n",
        "\n",
        "    Based on gymnasium environments\n",
        "    Action space is defined in classes inheriting from HedgingEnv.\n",
        "    Reward corresponds to wealth.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.trading_freq\n",
        "    self.trading_cost\n",
        "    self.strike\n",
        "    self.res_maturities\n",
        "    self.underlying_prices\n",
        "    self.options_prices\n",
        "    self.option_deltas\n",
        "    self.n_trajectory\n",
        "    self.n_step\n",
        "    self.index_trajectory\n",
        "    self.index_step\n",
        "    self.state_dim\n",
        "    self.state\n",
        "    self.reward\n",
        "    self.done\n",
        "    self.info\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(self)\n",
        "    reset(self)\n",
        "    step(self, action)\n",
        "    plot(self)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "        option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "        trading_params = (1, 1/100)\n",
        "        ):\n",
        "\n",
        "        # Parameters\n",
        "        _, self.strike, _, _, _, _ = option_params\n",
        "        self.trading_freq, self.trading_cost = trading_params\n",
        "\n",
        "        # Observation Space\n",
        "        underlying_prices, option_prices, option_deltas, res_maturities = get_financial_data(\n",
        "            underlying_params, option_params, trading_params\n",
        "        )\n",
        "\n",
        "        self.underlying_prices = underlying_prices\n",
        "        self.option_prices = option_prices\n",
        "        self.option_deltas = option_deltas\n",
        "        self.res_maturities = res_maturities\n",
        "\n",
        "        self.n_trajectory = underlying_prices.shape[0]\n",
        "        self.n_step = underlying_prices.shape[1]\n",
        "\n",
        "        # State\n",
        "        self.index_trajectory =  -1\n",
        "        self.index_step = -1\n",
        "\n",
        "        self.state_dim = 3\n",
        "        self.state = np.array([])\n",
        "\n",
        "        # Reward\n",
        "        self.reward = 0\n",
        "        self.rewards = []\n",
        "\n",
        "        # Done\n",
        "        self.done = False\n",
        "\n",
        "        # Info\n",
        "        self.info = [self.index_trajectory,self.index_step]\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environments.\n",
        "\n",
        "        Here resetting corresponds to a change of trajectory.\n",
        "        \"\"\"\n",
        "\n",
        "        # Change trajectory\n",
        "        self.index_trajectory = (self.index_trajectory + 1) % self.n_trajectory\n",
        "        self.index_step = 0\n",
        "\n",
        "        # Reset state\n",
        "        self.state = np.array([\n",
        "            self.underlying_prices[self.index_trajectory, self.index_step],\n",
        "            self.index_step,\n",
        "            self.res_maturities[self.index_step]\n",
        "        ])\n",
        "\n",
        "        # Reward (setting up the hedge at t=0)\n",
        "        underlying_price = self.underlying_prices[self.index_trajectory,self.index_step]\n",
        "        option_price = self.option_prices[self.index_trajectory,self.index_step]\n",
        "        option_delta = self.option_deltas[self.index_trajectory,self.index_step]\n",
        "\n",
        "        self.reward = \\\n",
        "            option_price \\\n",
        "            - self.trading_cost * np.abs(underlying_price * option_delta)\n",
        "        self.rewards = [self.reward]\n",
        "\n",
        "        # Done\n",
        "        self.done = False\n",
        "\n",
        "        # Info\n",
        "        self.info = [self.index_trajectory, self.index_step]\n",
        "\n",
        "        return self.state\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        # State\n",
        "        underlying_price = self.state[0]\n",
        "        position = self.state[1]\n",
        "        option_price = self.option_prices[self.index_trajectory, self.index_step]\n",
        "\n",
        "        # New state\n",
        "        new_index_step = self.index_step + 1\n",
        "        new_underlying_price = self.underlying_prices[self.index_trajectory, new_index_step]\n",
        "        new_position = action\n",
        "        new_option_price = self.option_prices[self.index_trajectory, new_index_step]\n",
        "        new_res_maturity = self.res_maturities[new_index_step]\n",
        "\n",
        "        new_state = np.array([new_underlying_price, new_position, new_res_maturity])\n",
        "\n",
        "        # Reward (liquidation of the hedge at t=n)\n",
        "        if new_index_step == self.n_step - 1:\n",
        "            new_option_price = np.max(underlying_price - self.strike, 0) # (St-K)+\n",
        "            self.done = True\n",
        "        else:\n",
        "            new_option_price = self.option_prices[self.index_trajectory, new_index_step]\n",
        "\n",
        "        new_reward = new_option_price - option_price\\\n",
        "            + position * (new_underlying_price - underlying_price)\\\n",
        "            - self.trading_cost * np.abs(new_underlying_price * (new_position - position))\n",
        "\n",
        "        self.rewards.append(new_reward)\n",
        "\n",
        "        # Updates\n",
        "        self.index_step = new_index_step\n",
        "        self.state = new_state\n",
        "        self.reward = new_reward\n",
        "        self.info = [self.index_trajectory,self.index_step]\n",
        "\n",
        "        return self.state, self.reward, self.done, self.info\n",
        "\n",
        "\n",
        "    def plot(self):\n",
        "        \"\"\"Plots current state\"\"\"\n",
        "\n",
        "        if self.index_step == -1:\n",
        "            plt.plot(self.underlying_prices.T)\n",
        "            plt.title('Simulation')\n",
        "            plt.xlabel('Steps')\n",
        "            plt.ylabel('Underlying Price')\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            # Plot\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
        "\n",
        "            axes[0].plot(self.underlying_prices[self.index_trajectory].T)\n",
        "            axes[0].set_title(f\"Trajectory {self.index_trajectory}\")\n",
        "            axes[0].set_xlabel('Step')\n",
        "            axes[0].set_ylabel('Underlying Price')\n",
        "            axes[0].axvline(self.index_step, c='red')\n",
        "            axes[0].legend([f\"Position: {self.state[1]}\",\n",
        "                            f\"Step: {self.index_step}\"])\n",
        "\n",
        "            axes[1].plot(self.rewards)\n",
        "            axes[1].set_title('Rewards')\n",
        "            axes[1].set_xlabel('Step')\n",
        "            axes[1].set_ylabel('Reward')\n",
        "            axes[1].legend([f\"Reward: {np.round(self.reward,2)}\"])\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "class ContinuousActionHedgingEnv(HedgingEnv):\n",
        "    \"\"\"Hedging env with continuous action space\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.action_space\n",
        "    self.action_meaning\n",
        "    self.action_dim\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __init__(self)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "        option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "        trading_params = (1, 1/100)\n",
        "        ):\n",
        "\n",
        "        super().__init__(underlying_params, option_params, trading_params)\n",
        "\n",
        "        # Action space\n",
        "        self.action_space = spaces.Box(low=np.array([0]), high=np.array([100]), dtype=np.float32)\n",
        "        self.action_dim = 1\n",
        "\n",
        "\n",
        "class DiscreteActionHedgingEnv(HedgingEnv):\n",
        "    \"\"\"Hedging env with discrete action space\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.action_space\n",
        "    self.action_meaning\n",
        "    self.action_dim\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    self.__init__()\n",
        "    self.step()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "        option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "        trading_params = (1, 1/100),\n",
        "        action_list=[0,25,50,100]\n",
        "        ):\n",
        "\n",
        "        super().__init__(underlying_params, option_params, trading_params)\n",
        "\n",
        "        # Action space\n",
        "        self.action_dim = len(action_list)\n",
        "        self.action_space = spaces.Discrete(self.action_dim)\n",
        "\n",
        "        self.action_mapping = {}\n",
        "        for index, value in enumerate(action_list):\n",
        "            self.action_mapping[index] = value\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        if action in range(self.action_dim):\n",
        "            return super().step(self.action_mapping[action])\n",
        "        else:\n",
        "            return super().step(action)"
      ],
      "metadata": {
        "id": "AGukr8UWu6EY"
      },
      "id": "AGukr8UWu6EY",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### instanciation"
      ],
      "metadata": {
        "id": "5f-HpG_8DbbA"
      },
      "id": "5f-HpG_8DbbA"
    },
    {
      "cell_type": "code",
      "source": [
        "# S0 = 100\n",
        "# mu = 5/100\n",
        "# sigma = 20/100\n",
        "# n = 252 # will correspond to the year base in days in the function\n",
        "# N = 100\n",
        "# underlying_params = (S0, mu, sigma, n, N)\n",
        "\n",
        "# S = 100\n",
        "# K = 100\n",
        "# r = 0\n",
        "# q = 0\n",
        "# T = 21\n",
        "# option_params = (S, K, r, q, sigma, T)\n",
        "\n",
        "# trading_freq = 1\n",
        "# trading_cost = 1/100\n",
        "# trading_params = (trading_freq, trading_cost)\n",
        "\n",
        "# env = DiscreteActionHedgingEnv(underlying_params, option_params, trading_params)"
      ],
      "metadata": {
        "id": "Di2JasORTrP7"
      },
      "id": "Di2JasORTrP7",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### explanations\n",
        "\n",
        "At $t_0$:\n",
        "- Spot\n",
        "- Option value\n",
        "- Initial position = Option Delta\n",
        "> Initial wealth = Option value - Trading cost * Initial position\n",
        "\n",
        "*Action (position for next period)*\n",
        "\n",
        "At $t+1$:\n",
        "- New spot\n",
        "- New option price\n",
        "- PNL (reward)"
      ],
      "metadata": {
        "id": "07VYvyLvrXYW"
      },
      "id": "07VYvyLvrXYW"
    },
    {
      "cell_type": "code",
      "source": [
        "# env.reset()\n",
        "\n",
        "# print(f\"Spot: {env.underlying_prices[env.index_trajectory, env.index_step]}\")\n",
        "# print(f\"Option: {env.option_prices[env.index_trajectory, env.index_step]}\")\n",
        "# print(f\"Position: {env.option_deltas[env.index_trajectory, env.index_step]}\")\n",
        "# print(f\"Wealth: {env.reward}\")\n",
        "\n",
        "# while not env.done:\n",
        "#     print('\\n')\n",
        "#     action = env.option_deltas[env.index_trajectory, env.index_step]\n",
        "#     step, reward, done, info = env.step(action)\n",
        "#     print(f\"ACTION (position for next period): {action}\")\n",
        "#     print('\\n')\n",
        "\n",
        "#     print(f\"Spot: {env.underlying_prices[env.index_trajectory, env.index_step]}\")\n",
        "#     print(f\"Option: {env.option_prices[env.index_trajectory, env.index_step]}\")\n",
        "#     print(f\"Wealth: {env.reward}\")\n"
      ],
      "metadata": {
        "id": "BZ3sgQXXlQ4e"
      },
      "id": "BZ3sgQXXlQ4e",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "gCpdp8lhwKTm"
      },
      "id": "gCpdp8lhwKTm"
    },
    {
      "cell_type": "code",
      "source": [
        "# # test global\n",
        "# env = DiscreteActionHedgingEnv(underlying_params, option_params, trading_params)\n",
        "\n",
        "# env.reset()\n",
        "# steps = []\n",
        "# rewards = []\n",
        "\n",
        "# for i in range(0,int(T/trading_freq)):\n",
        "\n",
        "#     action = env.action_space.sample().item()\n",
        "#     steps.append(env.step(action))\n",
        "#     rewards.append(env.reward)\n",
        "\n",
        "#     if i == 10:\n",
        "#         env.plot()"
      ],
      "metadata": {
        "id": "BjLlVbaf5I_S"
      },
      "id": "BjLlVbaf5I_S",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discrete action space - DQN\n",
        "References:\n",
        "- 2013, Mnih & al - Playing Atari with Reinforcement Learning\n",
        "- https://github.com/algila/Deep-Q-Learning-DQN\n",
        "- G.Turinici notebooks inspired from the above\n",
        "- https://github.com/tdmdal/rl-hedge-2019/tree/master\n",
        "\n",
        "\n",
        "**Action space**\n",
        "Action are in ${25,50,75}$\n"
      ],
      "metadata": {
        "id": "i4zF7-PcWvym"
      },
      "id": "i4zF7-PcWvym"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "Kwe-n9Xb0fzv"
      },
      "id": "Kwe-n9Xb0fzv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Paremeters as per Table 1 in the article\n",
        "TRADING_FREQ = 1\n",
        "TRADING_COST = 1/100\n",
        "TRADING_PARAMS = (TRADING_FREQ,\n",
        "                  TRADING_COST)\n",
        "\n",
        "# Parameters for underlying simulation\n",
        "UNDERLYING_INITIAL_PRICE = 100\n",
        "EXPECTED_RETURN = 5/100\n",
        "VOLATILITY = 20/100\n",
        "YEAR_BASE = 252\n",
        "TRAJECTORY_N = 50000\n",
        "UNDERLYING_PARAMS = (UNDERLYING_INITIAL_PRICE,\n",
        "                     EXPECTED_RETURN,\n",
        "                     VOLATILITY,\n",
        "                     YEAR_BASE,\n",
        "                     TRAJECTORY_N)\n",
        "\n",
        "# Paremeters for option computation\n",
        "STRIKE = 100\n",
        "RISK_FREE_RATE = 0\n",
        "DIVIDEND_YIELD = 0\n",
        "MATURITY = 21\n",
        "OPTION_PARAMS = (UNDERLYING_INITIAL_PRICE,\n",
        "                 STRIKE,\n",
        "                 RISK_FREE_RATE,\n",
        "                 DIVIDEND_YIELD,\n",
        "                 VOLATILITY,\n",
        "                 MATURITY)\n",
        "\n",
        "# Parameters for Replay Memory\n",
        "REPLAY_MEMORY_SIZE = 6000 # 600000\n",
        "MINIBATCH_SIZE = 64 # 128\n",
        "\n",
        "# Parameters for DQN\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Parameters for main loop\n",
        "epsilon = 1.0\n",
        "EPSILON_DECAY = 0.99994\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY_START = 25 # 2500\n",
        "epsilon_params = (epsilon,\n",
        "                  EPSILON_DECAY,\n",
        "                  EPSILON_MIN,\n",
        "                  EPSILON_DECAY_START)\n",
        "\n",
        "EPISODE_N = 500 # 50000\n",
        "EPISODE_N_TEST = 1000 # 10000\n",
        "TARGET_MODEL_UPDATE_FREQUENCY = 25 # 2500\n",
        "STATS_FREQUENCY = 10 # 1000\n",
        "\n",
        "MODEL_NAME = 'DQN - small run'\n",
        "WEIGHT_SAVE_FREQUENCY = 10 # 1000\n",
        "\n",
        "# Evaluation parameters\n",
        "RISK_ADVERSE_CONSTANT = 1.5\n",
        "EVALUATION_PARAMETERS = (RISK_ADVERSE_CONSTANT)"
      ],
      "metadata": {
        "id": "O9OFmAm5eyME"
      },
      "id": "O9OFmAm5eyME",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Memory\n",
        "$$ e_t = (s_t, a_t, r_{t+1}, s_{t+1}) $$\n",
        "\n",
        "> https://github.com/algila/Deep-Q-Learning-DQN/blob/main/DQN_paper_2013.py\n",
        "> https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb"
      ],
      "metadata": {
        "id": "BBgy5oFilu6b"
      },
      "id": "BBgy5oFilu6b"
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"Replay Memory that stores the last \"size\" experiences\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.buffer: List\n",
        "    self.size: int\n",
        "        Number of stored experiences\n",
        "    self.batch_size: int\n",
        "        Number of experiences returned in a minibatch\n",
        "    self.experience_index: int\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    self.__init__\n",
        "    self.__len__\n",
        "    self.add\n",
        "    self.get\n",
        "    self.sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        size = 600000,\n",
        "        batch_size = 128\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Number of stored experiences\n",
        "        batch_size: int\n",
        "            Number of experiences returned in a minibatch\n",
        "        \"\"\"\n",
        "\n",
        "        self.buffer = []\n",
        "        self.size = size\n",
        "        self.batch_size = batch_size\n",
        "        self.new_experience_index = 0\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "    def add(self, state, action, new_reward, new_state, done):\n",
        "        \"\"\"Adds an experiences to the buffer\"\"\"\n",
        "\n",
        "        new_experience = (state, action, new_reward, new_state, done)\n",
        "\n",
        "        if self.new_experience_index >= self.__len__():\n",
        "            self.buffer.append(new_experience)\n",
        "        else:\n",
        "            self.buffer[self.new_experience_index] = new_experience\n",
        "\n",
        "        self.new_experience_index = (self.new_experience_index + 1) % self.size\n",
        "\n",
        "\n",
        "    def get(self, index):\n",
        "        \"\"\"Gets an experience from the buffer\"\"\"\n",
        "\n",
        "        if self.__len__()==0:\n",
        "            raise ValueError(\"Replay memory is empty!\")\n",
        "\n",
        "        try:\n",
        "           return self.buffer[index]\n",
        "        except:\n",
        "          print('No such index in Replay Memory')\n",
        "          return None\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Sample a batch of experiences of size ```batch_size``` from ```buffer```.\"\"\"\n",
        "\n",
        "        # Check if enough experiences\n",
        "        if self.__len__() < self.batch_size:\n",
        "            raise ValueError('Not enough experiences in buffer to get a minibatch')\n",
        "\n",
        "        # Get random indexes\n",
        "        indexes = [random.randint(0, self.__len__() - 1) for _ in range(self.batch_size)]\n",
        "\n",
        "        # Get corresponding experiences\n",
        "        state_list, action_list, new_reward_list, new_state_list, done_list = [], [], [], [], []\n",
        "\n",
        "        for i in indexes:\n",
        "\n",
        "            experience = self.buffer[i]\n",
        "            state, action, new_reward, new_state, done = experience\n",
        "\n",
        "            state_list.append(np.array(state, copy=False))\n",
        "            action_list.append(action)\n",
        "            new_reward_list.append(new_reward)\n",
        "            new_state_list.append(np.array(new_state, copy=False))\n",
        "            done_list.append(done)\n",
        "\n",
        "        return np.array(state_list), np.array(action_list), np.array(new_reward_list), np.array(new_state_list), np.array(done_list)\n"
      ],
      "metadata": {
        "id": "dwOv1Xq-luR9"
      },
      "id": "dwOv1Xq-luR9",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "LWuoULX0tBw9"
      },
      "id": "LWuoULX0tBw9"
    },
    {
      "cell_type": "code",
      "source": [
        "# replay_memory = ReplayMemory(size=REPLAY_MEMORY_SIZE, batch_size=MINIBATCH_SIZE)\n",
        "\n",
        "# env = DiscreteActionHedgingEnv(underlying_params, option_params, trading_params)\n",
        "\n",
        "# env.reset()\n",
        "# state = env.state\n",
        "\n",
        "# for j in tqdm(range(0,10000)):\n",
        "#     for i in range(0,int(T/trading_freq)):\n",
        "#         action = env.action_space.sample().item()\n",
        "#         new_state, new_reward, done, info = env.step(action)\n",
        "#         replay_memory.add(state, action, new_reward, new_state, done)\n",
        "#         state = new_state\n",
        "#     env.reset()\n",
        "\n",
        "# print(len(replay_memory))\n",
        "\n",
        "# a, b, c, d, e = replay_memory.sample()\n",
        "# print(a.shape)"
      ],
      "metadata": {
        "id": "a9VZnnT6K44t"
      },
      "id": "a9VZnnT6K44t",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replay_memory = ReplayMemory(\n",
        "#     size=REPLAY_MEMORY_SIZE,\n",
        "#     batch_size=MINIBATCH_SIZE\n",
        "# )\n",
        "\n",
        "# env = ContinuousActionHedgingEnv()\n",
        "\n",
        "# env.reset()\n",
        "# state = env.state\n",
        "\n",
        "# for j in tqdm(range(0,10000)):\n",
        "#     for i in range(0,21):\n",
        "#         action = env.action_space.sample().item()\n",
        "#         new_state, new_reward, done, info = env.step(action)\n",
        "#         replay_memory.add(state, action, new_reward, new_state, done)\n",
        "#         state = new_state\n",
        "#     env.reset()\n",
        "\n",
        "# print(len(replay_memory))\n",
        "\n",
        "# a, b, c, d, e = replay_memory.sample()\n",
        "# print(a.shape)"
      ],
      "metadata": {
        "id": "OI5YeE7Xw_zx"
      },
      "id": "OI5YeE7Xw_zx",
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN Agent"
      ],
      "metadata": {
        "id": "HzQZts9hlpvJ"
      },
      "id": "HzQZts9hlpvJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class DQNAgent():\n",
        "    \"\"\"Deep Q-Network agent\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.env\n",
        "    self.state_dim\n",
        "    self.action_dim\n",
        "    self.discount_factor\n",
        "    self.learning_rate\n",
        "    self.model\n",
        "    self.target_model\n",
        "    self.target_update_counter\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    self._init_\n",
        "    self.create_model\n",
        "    self.describe_model\n",
        "    self.train\n",
        "    self.update_target_model\n",
        "    self.get_q\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        learning_rate,\n",
        "        discount_factor\n",
        "        ):\n",
        "\n",
        "        # Dimensions (for neural network)\n",
        "        self.state_dim = env.state_dim\n",
        "        self.action_dim = env.action_dim\n",
        "\n",
        "        # Parameters\n",
        "        self.discount_factor = discount_factor\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Main model\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        # Target network\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # Used to count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"Creates Deep Learning model\n",
        "\n",
        "        Hidden layers activation:\n",
        "        - relu\n",
        "        Output layer activation:\n",
        "        - sigmoid (for binary classification)\n",
        "        - softmax (for multi-class classification)\n",
        "        - linear (for regression problems with continuous values to be predicted)\n",
        "\n",
        "        Loss:\n",
        "        - mse (for regression tasks)\n",
        "        - categorical_crossentropy (for multi-class classification)\n",
        "        \"\"\"\n",
        "\n",
        "        model = Sequential() # weights are randomly initialized when new instance is created\n",
        "\n",
        "        model.add(Dense(32, input_dim=self.state_dim))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Dense(64))\n",
        "        model.add(Activation('relu'))\n",
        "\n",
        "        model.add(Dense(self.action_dim))\n",
        "        model.add(Activation('softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(self.learning_rate), loss='categorical_crossentropy')\n",
        "\n",
        "        # optimizer automatically computes the gradient and update model weights\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def describe_model(self):\n",
        "        print(self.model.summary())\n",
        "\n",
        "\n",
        "    def train(self, minibatch):\n",
        "\n",
        "        # Read minibatch\n",
        "        state_list, action_list, new_reward_list, new_state_list, done_list = minibatch\n",
        "        minibatch_size = len(state_list)\n",
        "\n",
        "        # Compute Q_true(St+1) et Q_ann(St)\n",
        "        Q_current_states = self.model.predict(state_list, verbose=0)\n",
        "        Q_future_states = self.target_model.predict(new_state_list, verbose=0)\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        # Enumerate batches\n",
        "        for step in range(MINIBATCH_SIZE):\n",
        "            state = state_list[step]\n",
        "            action = action_list[step]\n",
        "            new_reward = new_reward_list[step]\n",
        "            new_state = new_state_list[step]\n",
        "            done = done_list[step]\n",
        "\n",
        "            # Bellman equation.  Q = reward + gamma*max Q',\n",
        "\n",
        "            # Update Q-target\n",
        "            if not done:\n",
        "                new_Q_value = new_reward + self.discount_factor * np.max(Q_future_states[step])\n",
        "            else:\n",
        "                new_Q_value = new_reward\n",
        "\n",
        "            # Update estimated Q-values\n",
        "            Q_current_states[step][action] = new_Q_value\n",
        "\n",
        "            # And append to our training data\n",
        "            X.append(state)\n",
        "            Y.append(Q_current_states[step])\n",
        "\n",
        "        # Fit on all samples as one batch, NO log file saved. Quicker simulation\n",
        "        # Fit computes propagation, loss, backpropagation and weight update\n",
        "        self.model.fit(np.array(X), np.array(Y),\n",
        "                       batch_size=minibatch_size,\n",
        "                       verbose=1,\n",
        "                       epochs=1,\n",
        "                       shuffle=False)\n",
        "\n",
        "\n",
        "    # update target model function frames as verified into main (9*)\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def get_q(self, state):\n",
        "      return self.model.predict(np.array(state).reshape(1,-1))\n"
      ],
      "metadata": {
        "id": "C7lF6sd-ezSz"
      },
      "id": "C7lF6sd-ezSz",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "ddoUJctLBHXq"
      },
      "id": "ddoUJctLBHXq"
    },
    {
      "cell_type": "code",
      "source": [
        "# env = DiscreteActionHedgingEnv(UNDERLYING_PARAMS, OPTION_PARAMS, TRADING_PARAMS)\n",
        "\n",
        "# agent = DQNAgent(\n",
        "#   env=env,\n",
        "#   learning_rate=LEARNING_RATE,\n",
        "#   discount_factor=DISCOUNT_FACTOR\n",
        "# )\n",
        "\n",
        "# agent.describe_model()\n",
        "\n",
        "# agent.train(replay_memory.sample())\n",
        "\n",
        "# state = (110, 25, 10)\n",
        "# agent.get_q(state)"
      ],
      "metadata": {
        "id": "t0txAb64myX1"
      },
      "id": "t0txAb64myX1",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm\n",
        "\n",
        "> Mnih and al (2013)"
      ],
      "metadata": {
        "id": "Xra4gqCGl0MX"
      },
      "id": "Xra4gqCGl0MX"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "class MainDQN():\n",
        "    \"\"\"This class represents Mnih & al (2013) algorithm\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "        option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "        trading_params = (1, 1/100),\n",
        "        replay_memory_size = 600000,\n",
        "        minibatch_size = 128,\n",
        "        learning_rate = 1e-4,\n",
        "        discount_factor = 0.99,\n",
        "        epsilon_params = (1.0, 0.9994, 0.01, 2500),\n",
        "        episode_n = 50000,\n",
        "        model_name = 'DQN',\n",
        "        stats_frequency = 1000,\n",
        "        weight_save_frequency = 1000,\n",
        "        target_model_update_frequency = 2500,\n",
        "        load_flag = False,\n",
        "        tag = ''\n",
        "        ):\n",
        "\n",
        "        # Environment\n",
        "        self.env = DiscreteActionHedgingEnv(\n",
        "            underlying_params,\n",
        "            option_params,\n",
        "            trading_params\n",
        "        )\n",
        "\n",
        "        # Replay Memory\n",
        "        self.replay_memory = ReplayMemory(\n",
        "            replay_memory_size,\n",
        "            minibatch_size\n",
        "        )\n",
        "\n",
        "        # Network\n",
        "        self.agent = DQNAgent(\n",
        "            self.env,\n",
        "            learning_rate ,\n",
        "            discount_factor\n",
        "        )\n",
        "        self.model_name = model_name\n",
        "        self.tag = tag\n",
        "        self.load_flag = load_flag\n",
        "        if load_flag:\n",
        "            self.load_weights()\n",
        "\n",
        "        # Algo\n",
        "        self.epsilon, self.epsilon_decay, self.epsilon_min, self.epsilon_decay_start = epsilon_params\n",
        "\n",
        "        self.episode_n = episode_n\n",
        "        self.episode_count = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.stats_frequency = stats_frequency\n",
        "        self.episode_final_reward_list = []\n",
        "        self.stats = {}\n",
        "\n",
        "        self.target_model_update_frequency = target_model_update_frequency\n",
        "        self.weight_save_frequency = weight_save_frequency\n",
        "\n",
        "\n",
        "    def load_weights(self):\n",
        "        \"\"\"Loads weights for model\"\"\"\n",
        "\n",
        "        if self.tag == \"\":\n",
        "            file = f\"Cao and al, 2019 - models/{self.model_name}.h5\"\n",
        "        else:\n",
        "            file = f\"Cao and al, 2019 - models/{self.model_name} - {self.tag}.h5\"\n",
        "\n",
        "        if os.path.exists(file):\n",
        "            self.agent.model.load_weights(file)\n",
        "            self.agent.target_model.load_weights(file)\n",
        "\n",
        "\n",
        "    def epsilon_greedy_action_discrete_case(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            action = self.env.action_space.sample().item()\n",
        "        else:\n",
        "            action = np.argmax(self.agent.get_q(state)) # [0]\n",
        "        return action\n",
        "\n",
        "\n",
        "    def epsilon_update(self):\n",
        "        if self.epsilon >= self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def get_stats(self):\n",
        "        if not self.episode_final_reward_list:\n",
        "            print('No stats for empty reward list.')\n",
        "        else:\n",
        "            self.stats['episode'] = self.episode_count\n",
        "            self.stats['average'] = np.mean(self.episode_final_reward_list)\n",
        "            self.stats['min'] = np.min(self.episode_final_reward_list)\n",
        "            self.stats['max'] = np.max(self.episode_final_reward_list)\n",
        "            self.stats['last'] = self.episode_final_reward_list[-1]\n",
        "            print(f\"Episode: {self.stats['episode']}\\nAverage Final Wealth: {self.stats['average']}\\nMin Final Wealth: {self.stats['min']}\\nMax Final Wealth: {self.stats['max']}\\nLast Final Wealth: {self.stats['last']}\")\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        if self.load_flag:\n",
        "            start = self.tag\n",
        "        else:\n",
        "            start = 1\n",
        "\n",
        "        for episode in tqdm(range(start, self.episode_n + 1)): # 1 episode = 1 trajectory\n",
        "\n",
        "            self.episode_count += 1\n",
        "\n",
        "            # 3.\n",
        "            # Initialize sequence\n",
        "            state = self.env.reset()\n",
        "            episode_reward_list = []\n",
        "\n",
        "            while not self.env.done: # done when meeting end of trajectory\n",
        "\n",
        "                self.step_count += 1\n",
        "\n",
        "                # 4.\n",
        "                # Select epsilon-greedy action\n",
        "                action = self.epsilon_greedy_action_discrete_case(state)\n",
        "\n",
        "                # 5. 6.\n",
        "                # Execute action\n",
        "                new_state, new_reward, done, info = self.env.step(action)\n",
        "\n",
        "                # 7.\n",
        "                # Store transition in replay memory\n",
        "                self.replay_memory.add(state, action, new_reward, new_state, done)\n",
        "\n",
        "                # 8.\n",
        "                # Sample random minibatch\n",
        "                if  len(self.replay_memory) > self.replay_memory.batch_size:\n",
        "                    minibatch = self.replay_memory.sample()\n",
        "                    self.agent.train(minibatch)\n",
        "                    del minibatch\n",
        "\n",
        "                # 9.\n",
        "                # Reset Q_hat every C steps\n",
        "                if  self.step_count % self.target_model_update_frequency == 0 and self.step_count > self.replay_memory.batch_size:\n",
        "                    self.agent.update_target_model()\n",
        "\n",
        "                # Close loop\n",
        "                state = new_state\n",
        "                episode_reward_list.append(new_reward)\n",
        "\n",
        "\n",
        "            # Decay epsilon. Only start after replay memory is over min size\n",
        "            if self.step_count > self.epsilon_decay_start:\n",
        "                self.epsilon_update()\n",
        "\n",
        "            # Follow-up\n",
        "            if episode % self.stats_frequency == 0 and episode > 1:\n",
        "                self.episode_final_reward_list.append(np.sum(episode_reward_list))\n",
        "                self.get_stats()\n",
        "\n",
        "            # Save weights\n",
        "            if episode % self.weight_save_frequency == 0:\n",
        "                self.agent.model.save(f\"Cao and al, 2019 - models/{self.model_name} - {episode}.h5\")\n",
        "\n",
        "\n",
        "        self.agent.model.save(f\"Cao and al, 2019 - models/{self.model_name}.h5\")\n",
        "\n",
        "\n",
        "    def test(\n",
        "        self,\n",
        "        delta_action_flag = False,\n",
        "        episode_n = 10000,\n",
        "        risk_adverse_constant = 1.5,\n",
        "        verbose_frequency = 1000\n",
        "        ):\n",
        "        \"\"\"Test should be run in another instance\"\"\"\n",
        "\n",
        "        # Reinitialize\n",
        "        self.epsilon = -1\n",
        "        episode_index = 0\n",
        "\n",
        "        for episode in tqdm(range(1, episode_n + 1)): # 1 episode = 1 trajectory\n",
        "\n",
        "            episode_index += 1\n",
        "\n",
        "            # Initialize\n",
        "            state = self.env.reset()\n",
        "            episode_action_list = []\n",
        "            episode_reward_list = []\n",
        "\n",
        "            while not self.env.done:\n",
        "\n",
        "                # Select action\n",
        "                if delta_action_flag:\n",
        "                    action = self.env.option_deltas[self.env.index_trajectory,self.env.index_step] * 100\n",
        "                else:\n",
        "                    action = self.epsilon_greedy_action_discrete_case(state)\n",
        "\n",
        "                # Take action\n",
        "                new_state, new_reward, done, info = self.env.step(action)\n",
        "\n",
        "                # Close loop\n",
        "                episode_action_list.append(action)\n",
        "                episode_reward_list.append(new_reward)\n",
        "                state = new_state\n",
        "\n",
        "            # Follow-up\n",
        "            self.episode_final_reward_list.append(np.sum(episode_reward_list))\n",
        "\n",
        "            if episode % verbose_frequency == 0:\n",
        "                R_final_mean = np.mean(self.episode_final_reward_list)\n",
        "                R_final_var = np.var(self.episode_final_reward_list)\n",
        "                Y = -R_final_mean + risk_adverse_constant*np.sqrt(R_final_var)\n",
        "\n",
        "                print(f\"\\nEpisode {episode} final reward is: {self.episode_final_reward_list[-1]}\")\n",
        "                print(f\"Final reward mean: {R_final_mean}\")\n",
        "                print(f\"Final reward var: {R_final_var}\")\n",
        "                print(f\"Y = {Y}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j2gGv86Ln3d_"
      },
      "id": "j2gGv86Ln3d_",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### small run - train"
      ],
      "metadata": {
        "id": "toWCZCFWltMy"
      },
      "id": "toWCZCFWltMy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters 'small run'\n",
        "# algo = MainDQN(\n",
        "#     underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "#     option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "#     trading_params = (1, 1/100),\n",
        "#     replay_memory_size = 6000,\n",
        "#     minibatch_size = 64,\n",
        "#     learning_rate = 1e-4,\n",
        "#     discount_factor = 0.99,\n",
        "#     epsilon_params =(1.0, 0.99994, 0.01, 25),\n",
        "#     episode_n = 500,\n",
        "#     model_name = 'DQN - small run',\n",
        "#     stats_frequency = 10,\n",
        "#     weight_save_frequency = 10,\n",
        "#     target_model_update_frequency = 25\n",
        "# )\n",
        "\n",
        "# algo.train()"
      ],
      "metadata": {
        "id": "X5S_j5U1NhPK"
      },
      "id": "X5S_j5U1NhPK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### small run - test"
      ],
      "metadata": {
        "id": "v4oBsayXFPBI"
      },
      "id": "v4oBsayXFPBI"
    },
    {
      "cell_type": "code",
      "source": [
        "# algo = MainDQN(\n",
        "#     underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "#     option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "#     trading_params = (1, 1/100),\n",
        "#     replay_memory_size = 6000,\n",
        "#     minibatch_size = 64,\n",
        "#     learning_rate = 1e-4,\n",
        "#     discount_factor = 0.99,\n",
        "#     epsilon_params =(1.0, 0.99994, 0.01, 25),\n",
        "#     episode_n = 500,\n",
        "#     model_name = 'DQN - small run',\n",
        "#     stats_frequency = 10,\n",
        "#     weight_save_frequency = 10,\n",
        "#     target_model_update_frequency = 25,\n",
        "#     load_flag = True,\n",
        "#     tag = '' # no tag means final weights\n",
        "# )\n",
        "\n",
        "# algo.test(\n",
        "#     # delta_action_flag = False,\n",
        "#     delta_action_flag = True,\n",
        "#     episode_n = 100,\n",
        "#     risk_adverse_constant = 1.5,\n",
        "#     verbose_frequency = 10\n",
        "# )\n",
        "\n",
        "# DQN results (delta_action_flag = False)\n",
        "# Episode 100 final reward is: 318.0843837821259\n",
        "# Final reward mean: -95.04987783477348\n",
        "# Final reward var: 294240.9806954759\n",
        "# Y = 908.7096387697854\n",
        "\n",
        "# Delta hedging (delta_action_flag = True)\n",
        "# Episode 100 final reward is: -373.637076533253\n",
        "# Final reward mean: -160.5699514605786\n",
        "# Final reward var: 155184.62675484293\n",
        "# Y = 751.4721516582057"
      ],
      "metadata": {
        "id": "hkOD7e1VFReY"
      },
      "id": "hkOD7e1VFReY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full run"
      ],
      "metadata": {
        "id": "4PctmIIqK_md"
      },
      "id": "4PctmIIqK_md"
    },
    {
      "cell_type": "code",
      "source": [
        "# algo_train = MainDQN()\n",
        "\n",
        "# algo_test = MainDQN(load_flag = True)\n",
        "# algo_test.test(delta_action_flag = False)\n",
        "# algo_test.test(delta_action_flag = True)"
      ],
      "metadata": {
        "id": "LF6RKhymLCuq"
      },
      "id": "LF6RKhymLCuq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuous action space - Actor Critic\n",
        "\n",
        "References:\n",
        "> https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic?hl=fr\n",
        "\n",
        "\n",
        "\n",
        "**Greedy action**\n",
        "The greedy action $a$ is the one that minimizes:\n",
        "$$ F(S_t,a) = Q_1(S_t,a) + c \\cdot \\sqrt{ Q_2(S_t,a) - {Q_1(S_t,a)}^2} $$\n",
        "\n",
        "**Q-values**\n",
        "$Q_1(S_t, A_t; \\omega_1)$\n",
        "estimates the expected cost for state-action combination.\n",
        "$Q_2(S_t, A_t; \\omega_2)$ estimates the expected *square* cost for state-action combination.\n",
        "\n",
        "\n",
        "\n",
        "**Loss functions**\n",
        "\n",
        "$ L_1 = (R_{t+1} + \\gamma \\cdot Q_1(S_{t+1}, \\pi (S_{t+1})) - Q_1(S_t, A_t; \\omega_1))^2 $\n",
        "\n",
        "$ L_2 = (R^2_{t+1} + \\gamma^2 \\cdot Q_2(S_{t+1}, \\pi (S_{t+1})) + 2 \\cdot \\gamma \\cdot R_{t+1} \\cdot Q_1(S_{t+1},\\pi (S_{t+1})) - Q_2(S_t, A_t; \\omega_2))^2 $\n",
        "\n",
        "**Policy function**\n",
        "\n",
        "$\\pi (S_t;\\theta)$ is updated with:\n",
        "\n",
        "$$ \\theta  \\theta - \\alpha \\cdot \\nabla_{\\theta} F (S_t, \\pi(S_t;\\theta)) $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DsWXqhxrDd1q"
      },
      "id": "DsWXqhxrDd1q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Env"
      ],
      "metadata": {
        "id": "xhRTXtRdNAm5"
      },
      "id": "xhRTXtRdNAm5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "gm1r8IvNNQX8"
      },
      "id": "gm1r8IvNNQX8"
    },
    {
      "cell_type": "code",
      "source": [
        "# env = ContinuousActionHedgingEnv(\n",
        "#     underlying_params = (100, 5/100, 20/100, 252, 100),\n",
        "#     option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "#     trading_params = (1, 1/100)\n",
        "#     )\n",
        "\n",
        "# env.reset()\n",
        "\n",
        "# for i in range(0,20):\n",
        "\n",
        "#     print(f\"\\nInfo: {env.info}\")\n",
        "#     action = env.action_space.sample().item()\n",
        "#     print(f\"Action: {action}\")\n",
        "#     step = env.step(action)\n",
        "#     print(f\"State: {step}\")\n",
        "#     reward = env.reward\n",
        "#     print(reward)"
      ],
      "metadata": {
        "id": "drBlh_ahNGhf"
      },
      "id": "drBlh_ahNGhf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor Critic"
      ],
      "metadata": {
        "id": "4I5HFBj0MK48"
      },
      "id": "4I5HFBj0MK48"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Lambda, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "class ActorCritic():\n",
        "    \"\"\"Deep Q-Network agent\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.env\n",
        "    self.state_dim\n",
        "    self.action_dim\n",
        "    self.discount_factor\n",
        "    self.learning_rate\n",
        "    self.model\n",
        "    self.target_model\n",
        "    self.target_update_counter\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    self._init_\n",
        "    self.create_model\n",
        "    self.describe_model\n",
        "    self.train\n",
        "    self.update_target_model\n",
        "    self.get_q\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        learning_rate = 1e-4,\n",
        "        discount_factor = 0.99,\n",
        "        risk_adverse_constant = 1.5\n",
        "        ):\n",
        "\n",
        "        # Parameters\n",
        "        self.discount_factor = discount_factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.risk_adverse_constant = risk_adverse_constant\n",
        "\n",
        "        # Dimensions (for neural network)\n",
        "        self.state_dim = env.state_dim\n",
        "        self.action_dim = env.action_dim\n",
        "\n",
        "        # Networks\n",
        "        self.actor = self.create_actor()\n",
        "        self.Q1, self.Q2, self.F = self.create_critic()\n",
        "\n",
        "        # Target networks\n",
        "        self.actor_target = self.create_actor()\n",
        "        self.actor_target.set_weights(self.actor.get_weights())\n",
        "\n",
        "        self.Q1_target, self.Q2_target, self.F_target = self.create_critic()\n",
        "        self.Q1_target.set_weights(self.Q1.get_weights())\n",
        "        self.Q2_target.set_weights(self.Q2.get_weights())\n",
        "\n",
        "        # Used to count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "\n",
        "    def create_actor(self):\n",
        "        \"\"\"Creates Deep Learning model\n",
        "\n",
        "        Hidden layers activation:\n",
        "        - relu\n",
        "        Output layer activation:\n",
        "        - sigmoid (for binary classification)\n",
        "        - softmax (for multi-class classification)\n",
        "        - linear (for regression problems with continuous values to be predicted)\n",
        "\n",
        "        Loss:\n",
        "        - mse (for regression tasks)\n",
        "        - categorical_crossentropy (for multi-class classification)\n",
        "        \"\"\"\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Input(self.state_dim))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "        model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def create_critic(self):\n",
        "        \"\"\"Creates Deep Learning model\n",
        "\n",
        "        Hidden layers activation:\n",
        "        - relu\n",
        "        Output layer activation:\n",
        "        - sigmoid (for binary classification)\n",
        "        - softmax (for multi-class classification)\n",
        "        - linear (for regression problems with continuous values to be predicted)\n",
        "\n",
        "        Loss:\n",
        "        - mse (for regression tasks)\n",
        "        - categorical_crossentropy (for multi-class classification)\n",
        "        \"\"\"\n",
        "\n",
        "        state_input = Input(shape=(self.state_dim,))\n",
        "        action_input = Input(shape=(1,))\n",
        "\n",
        "        # Q1 will compute the expected cost of an action\n",
        "        Q1_input = concatenate([state_input, action_input])\n",
        "        Q1_hidden_layer_1 = Dense(32, activation='relu')(Q1_input)\n",
        "        Q1_hidden_layer_2 = Dense(64, activation='relu')(Q1_hidden_layer_1)\n",
        "        Q1_output_layer = Dense(1, activation='linear')(Q1_hidden_layer_2)\n",
        "\n",
        "        Q1 = Model(inputs = [state_input,action_input], outputs = Q1_output_layer)\n",
        "        Q1.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "\n",
        "        # Q2 will compute the square expected cost of an action\n",
        "        Q2_input = concatenate([state_input, action_input])\n",
        "        Q2_hidden_layer_1 = Dense(32, activation='relu')(Q2_input)\n",
        "        Q2_hidden_layer_2 = Dense(64, activation='relu')(Q2_hidden_layer_1)\n",
        "        Q2_output_layer = Dense(1, activation='linear')(Q2_hidden_layer_2)\n",
        "\n",
        "        Q2 = Model(inputs = [state_input,action_input], outputs = Q2_output_layer)\n",
        "        Q2.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "\n",
        "        # F (as refered to in the article)\n",
        "        # We build a model for F as F is function of Q1 and Q2\n",
        "        # and we will need to compute F gradient\n",
        "        F = Model(\n",
        "            inputs = [state_input, action_input],\n",
        "            outputs = Lambda(lambda Q: Q[0] - self.risk_adverse_constant * K.sqrt(K.max(Q[1] - Q[0]**2, 0)))\\\n",
        "                ([Q1_output_layer, Q2_output_layer])\n",
        "        )\n",
        "        F.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "\n",
        "        return Q1, Q2, F\n",
        "\n",
        "\n",
        "    def describe_model(self):\n",
        "        print('\\nActor')\n",
        "        print(self.actor.summary())\n",
        "        print('\\nCritic Q1')\n",
        "        print(self.Q1.summary())\n",
        "        print('\\nCritic Q2')\n",
        "        print(self.Q2.summary())\n",
        "        print('\\nF')\n",
        "        print(self.F.summary())\n",
        "\n",
        "\n",
        "    def train(self, minibatch):\n",
        "\n",
        "        # Read minibatch\n",
        "        state_array, action_array, new_reward_array, new_state_array, done_array = minibatch\n",
        "        minibatch_size = len(state_array)\n",
        "\n",
        "        # Get next action\n",
        "        next_action_array = self.actor_target.predict(new_state_array)\n",
        "\n",
        "        # Compute target Q-values\n",
        "        Q1_next_state = self.Q1_target.predict([new_state_array, next_action_array])\n",
        "        Q2_next_state = self.Q2_target.predict([new_state_array, next_action_array])\n",
        "\n",
        "        y_Q1 = new_reward_array \\\n",
        "            + (1-done_array) * self.discount_factor * Q1_next_state\n",
        "        y_Q2 = new_reward_array**2 \\\n",
        "            + (1-done_array) * self.discount_factor**2 * Q2_next_state \\\n",
        "            + (1-done_array) * 2 * self.discount_factor * new_reward_array * Q1_next_state\n",
        "\n",
        "        # Gradient_descent\n",
        "        # Note that by construction, F is fitted whenever Q1 or Q2 are fitted\n",
        "        self.Q1.fit(\n",
        "            [state_array, action_array],\n",
        "            y_Q1,\n",
        "            batch_size=minibatch_size,\n",
        "            verbose=0,\n",
        "            epochs=1,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        self.Q2.fit(\n",
        "            [state_array, action_array],\n",
        "            y_Q2,\n",
        "            batch_size=minibatch_size,\n",
        "            verbose=0,\n",
        "            epochs=1,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        self.update_actor(state_array)\n",
        "\n",
        "\n",
        "    def update_actor(self, state_array):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            action_array = self.actor(state_array)\n",
        "            F_output = self.F([state_array, action_array])\n",
        "\n",
        "        actor_gradients = tape.gradient(F_output, self.actor.trainable_variables)\n",
        "\n",
        "        actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
        "        actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
        "\n",
        "\n",
        "        # Method 1: Rebuild crtic gradient - NOT AVAILABLE IN EAGER MODE\n",
        "        # action_array = self.actor(state_array)\n",
        "        # F_input = self.F.input\n",
        "        # F_output = self.F.output\n",
        "        # F_gradient = K.gradients(F_input, F_output[1])\n",
        "        # F_gradient_function = K.function([F_input[0], F_input[1]], F_gradient)\n",
        "\n",
        "        # actor_gradient = np.array(F_gradient_function([state_array, action_array]))[0]\n",
        "\n",
        "        # actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
        "        # actor_optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))\n",
        "\n",
        "\n",
        "        # Method 2 - Track operations\n",
        "        # action_array = self.actor(state_array)\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     actor_gradient = tape.gradient(\n",
        "        #         self.F([state_array,action_array]),\n",
        "        #         self.actor.trainable_variables\n",
        "        #     )\n",
        "\n",
        "        # actor_optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
        "        # actor_optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))\n",
        "\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.Q1_target.set_weights(self.Q1.get_weights())\n",
        "        self.Q2_target.set_weights(self.Q2.get_weights())\n",
        "        self.actor_target.set_weights(self.actor.get_weights())\n",
        "\n",
        "\n",
        "    def save_weights(self, model_name, tag=None):\n",
        "        \"\"\"Saves model weights\"\"\"\n",
        "\n",
        "        if tag:\n",
        "            folder_path = f\"Cao and al, 2019 - models/{model_name} - {tag}/\"\n",
        "        else:\n",
        "            folder_path = f\"Cao and al, 2019 - models/{model_name}/\"\n",
        "\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        self.actor.save_weights(folder_path + \"Actor.h5\")\n",
        "        self.Q1.save_weights(folder_path + \"Q1.h5\")\n",
        "        self.Q2.save_weights(folder_path + \"Q2.h5\")\n",
        "\n",
        "\n",
        "    def load_weights(self, model_name, tag = None):\n",
        "        \"\"\"Loads model weights\"\"\"\n",
        "\n",
        "        if tag:\n",
        "            folder_path = f\"Cao and al, 2019 - models/{model_name} - {tag}/\"\n",
        "        else:\n",
        "            folder_path = f\"Cao and al, 2019 - models/{model_name}/\"\n",
        "\n",
        "        self.actor.load_weights(folder_path + \"Actor.h5\")\n",
        "        self.Q1.load_weights(folder_path + \"Q1.h5\")\n",
        "        self.Q2.load_weights(folder_path + \"Q2.h5\")"
      ],
      "metadata": {
        "id": "yPxPjY_oKWTu"
      },
      "id": "yPxPjY_oKWTu",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "GRwcPUofnDyL"
      },
      "id": "GRwcPUofnDyL"
    },
    {
      "cell_type": "code",
      "source": [
        "# # FILL REPLAY MEMORY\n",
        "# env = ContinuousActionHedgingEnv()\n",
        "# replay_memory = ReplayMemory(\n",
        "#     size=6000,\n",
        "#     batch_size=64\n",
        "# )\n",
        "# env.reset()\n",
        "# state = env.state\n",
        "# for j in tqdm(range(0,10000)):\n",
        "#     for i in range(0,21):\n",
        "#         action = env.action_space.sample().item()\n",
        "#         new_state, new_reward, done, info = env.step(action)\n",
        "#         replay_memory.add(state, action, new_reward, new_state, done)\n",
        "#         state = new_state\n",
        "#     env.reset()"
      ],
      "metadata": {
        "id": "b_HugFVTW-Gq"
      },
      "id": "b_HugFVTW-Gq",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ac = ActorCritic(env)\n",
        "# ac.describe_model()\n",
        "# ac.train(replay_memory.sample())\n",
        "\n",
        "# state = tf.expand_dims((110, 25, 10), axis=0)\n",
        "# ac.save_weights('ACtest')\n",
        "# print(ac.actor.predict(state)[0][0])\n",
        "\n",
        "# ac_test = ActorCritic(env)\n",
        "# ac_test.load_weights('ACtest')\n",
        "# print(ac_test.actor.predict(state)[0][0])"
      ],
      "metadata": {
        "id": "7MLwcH0OtvSn"
      },
      "id": "7MLwcH0OtvSn",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm"
      ],
      "metadata": {
        "id": "CgcXCk0FM53d"
      },
      "id": "CgcXCk0FM53d"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "class MainAC():\n",
        "    \"\"\"This class represents Mnih & al (2013) algorithm\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "        option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "        trading_params = (1, 1/100),\n",
        "        replay_memory_size = 600000,\n",
        "        minibatch_size = 128,\n",
        "        learning_rate = 1e-4,\n",
        "        discount_factor = 0.99,\n",
        "        risk_adverse_constant = 1.5,\n",
        "        epsilon_params = (1.0, 0.9994, 0.01, 2500),\n",
        "        episode_n = 50000,\n",
        "        model_name = 'ActorCritic',\n",
        "        stats_frequency = 1000,\n",
        "        weight_save_frequency = 1000,\n",
        "        target_model_update_frequency = 2500,\n",
        "        load_flag = False,\n",
        "        tag = ''\n",
        "        ):\n",
        "\n",
        "        # Environment\n",
        "        self.env = ContinuousActionHedgingEnv(\n",
        "            underlying_params,\n",
        "            option_params,\n",
        "            trading_params\n",
        "        )\n",
        "\n",
        "        # Replay Memory\n",
        "        self.replay_memory = ReplayMemory(\n",
        "            replay_memory_size,\n",
        "            minibatch_size\n",
        "        )\n",
        "\n",
        "        # Network\n",
        "        self.agent = ActorCritic(\n",
        "            self.env,\n",
        "            learning_rate ,\n",
        "            discount_factor,\n",
        "            risk_adverse_constant\n",
        "        )\n",
        "        self.model_name = model_name\n",
        "        self.tag = tag\n",
        "        self.load_flag = load_flag\n",
        "        if load_flag:\n",
        "            self.agent.load_weights(self.model_name, self.tag)\n",
        "\n",
        "        # Algo\n",
        "        self.epsilon, self.epsilon_decay, self.epsilon_min, self.epsilon_decay_start = epsilon_params\n",
        "\n",
        "        self.episode_n = episode_n\n",
        "        self.episode_count = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.stats_frequency = stats_frequency\n",
        "        self.episode_final_reward_list = []\n",
        "        self.stats = {}\n",
        "\n",
        "        self.target_model_update_frequency = target_model_update_frequency\n",
        "        self.weight_save_frequency = weight_save_frequency\n",
        "\n",
        "\n",
        "    def epsilon_greedy_action_continuous_case(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            action = self.env.action_space.sample().item()\n",
        "        else:\n",
        "            action = self.agent.actor.predict(state)[0][0]\n",
        "            # [0][0] to have only the action and not an array\n",
        "            action = np.clip(action, 0, 100)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def epsilon_update(self):\n",
        "        if self.epsilon >= self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def get_stats(self):\n",
        "        if not self.episode_final_reward_list:\n",
        "            print('No stats for empty reward list.')\n",
        "        else:\n",
        "            self.stats['episode'] = self.episode_count\n",
        "            self.stats['average'] = np.mean(self.episode_final_reward_list)\n",
        "            self.stats['min'] = np.min(self.episode_final_reward_list)\n",
        "            self.stats['max'] = np.max(self.episode_final_reward_list)\n",
        "            self.stats['last'] = self.episode_final_reward_list[-1]\n",
        "            print(f\"Episode: {self.stats['episode']}\\nAverage Final Wealth: {self.stats['average']}\\nMin Final Wealth: {self.stats['min']}\\nMax Final Wealth: {self.stats['max']}\\nLast Final Wealth: {self.stats['last']}\")\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        if self.load_flag:\n",
        "            start = self.tag\n",
        "        else:\n",
        "            start = 1\n",
        "\n",
        "        for episode in tqdm(range(start, self.episode_n + 1)): # 1 episode = 1 trajectory\n",
        "\n",
        "            self.episode_count += 1\n",
        "\n",
        "            # 3.\n",
        "            # Initialize sequence\n",
        "            state = self.env.reset()\n",
        "            episode_reward_list = []\n",
        "\n",
        "            while not self.env.done: # done when meeting end of trajectory\n",
        "\n",
        "                self.step_count += 1\n",
        "\n",
        "                # 4.\n",
        "                # Select epsilon-greedy action\n",
        "                action = self.epsilon_greedy_action_continuous_case(state)\n",
        "\n",
        "                # 5. 6.\n",
        "                # Execute action\n",
        "                new_state, new_reward, done, info = self.env.step(action)\n",
        "\n",
        "                # 7.\n",
        "                # Store transition in replay memory\n",
        "                self.replay_memory.add(state, action, new_reward, new_state, done)\n",
        "\n",
        "                # 8.\n",
        "                # Sample random minibatch\n",
        "                if  len(self.replay_memory) > self.replay_memory.batch_size:\n",
        "                    minibatch = self.replay_memory.sample()\n",
        "                    self.agent.train(minibatch)\n",
        "                    del minibatch\n",
        "\n",
        "                # 9.\n",
        "                # Reset Q_target every C steps\n",
        "                if  self.step_count % self.target_model_update_frequency == 0 and self.step_count > self.replay_memory.batch_size:\n",
        "                    self.agent.update_target_model()\n",
        "\n",
        "                # Close loop\n",
        "                state = new_state\n",
        "                episode_reward_list.append(new_reward)\n",
        "\n",
        "\n",
        "            # Decay epsilon. Only start after replay memory is over min size\n",
        "            if self.step_count > self.epsilon_decay_start:\n",
        "                self.epsilon_update()\n",
        "\n",
        "            # Follow-up\n",
        "            self.episode_final_reward_list.append(np.sum(episode_reward_list))\n",
        "            if episode % self.stats_frequency == 0 and episode > 1:\n",
        "                self.get_stats()\n",
        "\n",
        "            # Save weights\n",
        "            if episode % self.weight_save_frequency == 0:\n",
        "                self.agent.save_weights(self.model_name, episode)\n",
        "\n",
        "        self.agent.save_weights(self.model_name)\n",
        "\n",
        "\n",
        "    def test(\n",
        "        self,\n",
        "        delta_action_flag = False,\n",
        "        episode_n = 10000,\n",
        "        verbose_frequency = 1000\n",
        "        ):\n",
        "        \"\"\"Test should be run in another instance\"\"\"\n",
        "\n",
        "        # Reinitialize\n",
        "        self.epsilon = -1\n",
        "        episode_index = 0\n",
        "\n",
        "        for episode in tqdm(range(1, episode_n + 1)): # 1 episode = 1 trajectory\n",
        "\n",
        "            episode_index += 1\n",
        "\n",
        "            # Initialize\n",
        "            state = self.env.reset()\n",
        "            episode_action_list = []\n",
        "            episode_reward_list = []\n",
        "\n",
        "            while not self.env.done:\n",
        "\n",
        "                # Select action\n",
        "                if delta_action_flag:\n",
        "                    action = self.env.option_deltas[self.env.index_trajectory,self.env.index_step] * 100\n",
        "                else:\n",
        "                    action = self.epsilon_greedy_action_continuous_case(tf.expand_dims(state, axis=0))\n",
        "\n",
        "                # Take action\n",
        "                new_state, new_reward, done, info = self.env.step(action)\n",
        "\n",
        "                # Close loop\n",
        "                episode_action_list.append(action)\n",
        "                episode_reward_list.append(new_reward)\n",
        "                state = new_state\n",
        "\n",
        "            # Follow-up\n",
        "            self.episode_final_reward_list.append(np.sum(episode_reward_list))\n",
        "\n",
        "            if episode % verbose_frequency == 0:\n",
        "                R_final_mean = np.mean(self.episode_final_reward_list)\n",
        "                R_final_var = np.var(self.episode_final_reward_list)\n",
        "                Y = -R_final_mean + self.agent.risk_adverse_constant*np.sqrt(R_final_var)\n",
        "\n",
        "                print(f\"\\nEpisode {episode} final reward is: {self.episode_final_reward_list[-1]}\")\n",
        "                print(f\"Final reward mean: {R_final_mean}\")\n",
        "                print(f\"Final reward var: {R_final_var}\")\n",
        "                print(f\"Y = {Y}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XvpfZDaRnAhT"
      },
      "id": "XvpfZDaRnAhT",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### small run - train"
      ],
      "metadata": {
        "id": "7hjf05Qxwbf3"
      },
      "id": "7hjf05Qxwbf3"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Parameters 'small run'\n",
        "# algo_ac = MainAC(\n",
        "#     underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "#     option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "#     trading_params = (1, 1/100),\n",
        "#     replay_memory_size = 6000,\n",
        "#     minibatch_size = 64,\n",
        "#     learning_rate = 1e-4,\n",
        "#     discount_factor = 0.99,\n",
        "#     risk_adverse_constant=1.5,\n",
        "#     epsilon_params =(1.0, 0.99994, 0.01, 25),\n",
        "#     episode_n = 500,\n",
        "#     model_name = 'ActorCritic - small run',\n",
        "#     stats_frequency = 10,\n",
        "#     weight_save_frequency = 10,\n",
        "#     target_model_update_frequency = 25\n",
        "# )\n",
        "\n",
        "# algo_ac.train()"
      ],
      "metadata": {
        "id": "4XqeHj1DwYhj"
      },
      "id": "4XqeHj1DwYhj",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### small run - test"
      ],
      "metadata": {
        "id": "vZbNEWkZwpXx"
      },
      "id": "vZbNEWkZwpXx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters 'small run'\n",
        "# algo_ac_test = MainAC(\n",
        "#     underlying_params = (100, 5/100, 20/100, 252, 50000),\n",
        "#     option_params = (100, 100, 0, 0, 20/100, 21),\n",
        "#     trading_params = (1, 1/100),\n",
        "#     replay_memory_size = 6000,\n",
        "#     minibatch_size = 64,\n",
        "#     learning_rate = 1e-4,\n",
        "#     discount_factor = 0.99,\n",
        "#     risk_adverse_constant=1.5,\n",
        "#     epsilon_params =(1.0, 0.99994, 0.01, 25),\n",
        "#     episode_n = 500,\n",
        "#     model_name = 'ActorCritic - small run',\n",
        "#     stats_frequency = 10,\n",
        "#     weight_save_frequency = 10,\n",
        "#     target_model_update_frequency = 25,\n",
        "#     load_flag = True,\n",
        "#     tag = 40 # no tag means final weights\n",
        "# )\n",
        "\n",
        "# algo_ac_test.test(\n",
        "#     delta_action_flag = False,\n",
        "#     # delta_action_flag = True,\n",
        "#     episode_n = 100,\n",
        "#     verbose_frequency = 10\n",
        "# )\n",
        "\n",
        "# AC results (delta_action_flag = False)\n",
        "# Episode 100 final reward is: -3.731407951438186\n",
        "# Final reward mean: -3.528924405517586\n",
        "# Final reward var: 31.959103145300347\n",
        "# Y = 12.008781838100152\n",
        "\n",
        "# Delta hedging (delta_action_flag = True)\n",
        "# Episode 100 final reward is: -119.78820718706821\n",
        "# Final reward mean: -202.08242112906365\n",
        "# Final reward var: 122342.20017701518\n",
        "# Y = 726.7441696854876"
      ],
      "metadata": {
        "id": "7eMA1Ek5wsLF"
      },
      "id": "7eMA1Ek5wsLF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full run"
      ],
      "metadata": {
        "id": "hBWe-xGRJZ0B"
      },
      "id": "hBWe-xGRJZ0B"
    },
    {
      "cell_type": "code",
      "source": [
        "algo_train = MainAC()\n",
        "algo_train.train()\n",
        "\n",
        "algo_test = MainDQN(load_flag = True)\n",
        "algo_test.test(delta_action_flag = False)\n",
        "algo_test.test(delta_action_flag = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlLku20fGuDB",
        "outputId": "fb52f29f-d048-4682-ff7d-db84187d2064"
      },
      "id": "zlLku20fGuDB",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}