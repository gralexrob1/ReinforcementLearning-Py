{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ-CplzQuSTY"
      },
      "source": [
        "# DQN agent training on the Breakout Atari Game, may 2023\n",
        "\n",
        "by Gabriel Turinici\n",
        "\n",
        "\n",
        "Many parts of the code use the implementation from https://github.com/algila/Deep-Q-Learning-DQN.\n",
        "\n",
        "The code has been simplified (for instance the logging is not present).\n",
        "\n",
        "A part has been added to play the game using the trained network."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf9YYPww57sU"
      },
      "source": [
        "## Basic package setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MUIBiiJorAY3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (0.0.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[atari]) (1.23.5)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[atari]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[atari]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alexa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting ale-py~=0.8.1\n",
            "  Downloading ale_py-0.8.1-cp311-cp311-win_amd64.whl (952 kB)\n",
            "     ---------------------------------------- 0.0/952.4 kB ? eta -:--:--\n",
            "     -------------- ----------------------- 358.4/952.4 kB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------- ----------- 665.6/952.4 kB 7.0 MB/s eta 0:00:01\n",
            "     -------------------------------------  952.3/952.4 kB 7.5 MB/s eta 0:00:01\n",
            "     -------------------------------------- 952.4/952.4 kB 6.7 MB/s eta 0:00:00\n",
            "Collecting importlib-resources\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: importlib-resources, ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 importlib-resources-5.12.0 shimmy-0.2.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlOq1PvN5wyz",
        "outputId": "78434ad3-62b4-4940-e02a-e633f432e02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eagerly is running\n",
            "tensorflow version 2.12.0\n",
            "keras version 2.12.0\n"
          ]
        }
      ],
      "source": [
        "#v2 keep working in TF2.3.0\n",
        "#v3 added DQN and used a FIFO for the REPLAY_MEMORY (now it is working very slowly)\n",
        "#v4 used pre-processing 84x84x4 as for DeepMind and adapted DQN for that input\n",
        "#v5 improved ReplayMemory to use less RAM (derived from code fg91)\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras.backend as backend\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from tensorflow.python.framework.ops import enable_eager_execution\n",
        "\n",
        "from functools import partial\n",
        "print= partial(print, flush=True)#to always flush the output\n",
        "\n",
        "\n",
        "if tf.executing_eagerly():\n",
        "    print('Eagerly is running')\n",
        "else:\n",
        "    print('Eagerly is disables')\n",
        "\n",
        "print(f'tensorflow version {tf.__version__}')\n",
        "print(f'keras version {tf.keras.__version__}')\n",
        "\n",
        "#env = gym.make(\"MountainCar-v0\")\n",
        "#env = gym.make(\"Breakout-v4\")\n",
        "env = gym.make(\"ALE/Breakout-v5\",render_mode=\"rgb_array\")\n",
        "#env = gym.make(\"PongDeterministic-v4\")\n",
        "#env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IVvJXxQ650rx"
      },
      "source": [
        "## Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJrgoAn2-KBA",
        "outputId": "b2105856-27db-4aca-efb6-92901b97eb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "print('Set parameters')\n",
        "DEBUG = False\n",
        "SHOW_EVERY = 200\n",
        "\n",
        "DISCOUNT = 0.99                 # Discount factor gamma used in the Q-learning update\n",
        "LEARNING_RATE = 0.00025         # 0.00001   0.00025 Mnih et all 2015\n",
        "                                  # Hessel et al. 2017 used 0.0000625\n",
        "                                # in Pong use 0.00025\n",
        "REPLAY_MEMORY_SIZE = 1000000      #100000  # How many last steps to keep in memory for model training\n",
        "REPLAY_MEMORY_SIZE = 100000      #100000  # How many last steps to keep in memory for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = 50000   #50000  # Minimum number of random steps before to start training with the memory\n",
        "MIN_REPLAY_MEMORY_SIZE = 5000   #50000  # Minimum number of random steps before to start training with the memory\n",
        "                                # This is also the Number of completely random actions before the agent starts learning\n",
        "MAX_FRAMES = 25000000           #50milion # Total number of frames the agent sees during training\n",
        "MAX_FRAMES = 25000000           # Total number of frames the agent sees during training\n",
        "MINIBATCH_SIZE = 32             # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_MODEL =10000       #10000 # Number of chosen actions between updating the target network.\n",
        "UPDATE_TARGET_MODEL =100       #10000 # Number of chosen actions between updating the target network.\n",
        "\n",
        "                                       # According to Mnih et al. 2015 this is measured in the number of\n",
        "                                       # parameter updates (every four actions), however, in the\n",
        "                                       # DeepMind code, it is clearly measured in the number\n",
        "                                       # of actions the agent choses\n",
        "UPDATE_MODEL = 4\n",
        "\n",
        "MODEL_NAME = 'DQN_DeepMind'\n",
        "MIN_REWARD = -200  # For model save\n",
        "\n",
        "#EPISODES = 20000   # number of match played in total during training\n",
        "EPISODES = 2000   # number of match played in total during training\n",
        "\n",
        "SAVE_EPISODE_EVERY= int(EPISODES/10)#save model 10 times in the whole run\n",
        "\n",
        "# Exploration annealing settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "MIN_EPSILON = 0.01\n",
        "MAX_EPSILON = 1.00\n",
        "MID_EPSILON = 0.1\n",
        "\n",
        "EPSILON_DECAY_1 = (MAX_EPSILON-MID_EPSILON)/(REPLAY_MEMORY_SIZE-MIN_REPLAY_MEMORY_SIZE)\n",
        "EPSILON_DECAY_2 = (MID_EPSILON-MIN_EPSILON)/(MAX_FRAMES-REPLAY_MEMORY_SIZE)\n",
        "\n",
        "#  Stats settings\n",
        "AGGREGATE_STATS_EVERY = 100  # episodes/match\n",
        "\n",
        "# used to print on the terminal\n",
        "ep_rewards = []\n",
        "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TzKan_qk-LJr"
      },
      "source": [
        "## Define classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8zHzzQXZp_RI"
      },
      "outputs": [],
      "source": [
        "class DQNAgent():\n",
        "    def __init__(self):\n",
        "        # Main model\n",
        "        self.model = self.create_model()\n",
        "        print(self.model.summary())\n",
        "\n",
        "        # Target network\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # Used to count when to update target network with main network's weights\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, (8, 8), input_shape=[84, 84, 4], strides=4,\n",
        "                         kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))  # 4 frame greyscale 84x84\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(64, (4, 4), strides=2, kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), strides=1, kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "        model.add(Dense(512, kernel_initializer=keras.initializers.VarianceScaling(scale=2.0), activation='relu'))\n",
        "\n",
        "        model.add(Dense(env.action_space.n, activation='linear',\n",
        "                        kernel_initializer=keras.initializers.VarianceScaling(scale=2.0)))  # action_space = how many choices (2)\n",
        "        #model.compile(loss=\"mse\", optimizer=Adam(lr=0.00025), metrics=['accuracy'])\n",
        "        model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "    # Trains main network every step during episode\n",
        "    def train(self, minibatch):\n",
        "\n",
        "        # Get current states from minibatch, then query NN model for Q values\n",
        "        current_states = np.array(minibatch[0])/255\n",
        "        current_qs_list = self.model.predict(current_states,verbose=0)\n",
        "\n",
        "        # Get future states from minibatch, then query NN model for Q values\n",
        "        new_current_states = np.array(minibatch[3])/255\n",
        "        future_qs_list = self.target_model.predict(new_current_states,verbose=0)\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        # Now we need to enumerate our batches\n",
        "        for ii in range(MINIBATCH_SIZE):\n",
        "            current_state     = minibatch[0][ii]\n",
        "            action            = minibatch[1][ii]\n",
        "            reward            = minibatch[2][ii]\n",
        "            new_current_state = minibatch[3][ii]\n",
        "            done              = minibatch[4][ii]\n",
        "\n",
        "            # Bellman equation.  Q = r + gamma*max Q',\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
        "            # almost like with Q Learning, but we use just part of equation here\n",
        "            if not done:\n",
        "                new_q = reward + DISCOUNT * np.max(future_qs_list[ii])\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            # Update Q value for given state\n",
        "            current_qs = current_qs_list[ii]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # And append to our training data\n",
        "            X.append(current_state)\n",
        "            Y.append(current_qs)\n",
        "\n",
        "        # Fit on all samples as one batch, NO log file saved. Quicker simulation\n",
        "        # Note : not need to shuffle, this is already done in the get_minibatch part\n",
        "        self.model.fit(np.array(X), np.array(Y), batch_size=MINIBATCH_SIZE, \n",
        "                       verbose=0, epochs=1,shuffle=False)\n",
        "\n",
        "\n",
        "    # update target model function frames as verified into main (9*)\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    # Queries main network for Q values given current observation space (environment state)\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255,verbose=0)[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJmCU0qB9IB5"
      },
      "source": [
        "## Replay Memory class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Zc_nw90Q9Hhw"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):  # derived from https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\n",
        "    \"\"\"Replay Memory that stores the last \"size\" transitions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size=1000, frame_height=84, frame_width=84,\n",
        "                 agent_history_length=4, batch_size=32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            size: Integer, Number of stored transitions\n",
        "            frame_height: Integer, Height of a frame of an Atari game\n",
        "            frame_width: Integer, Width of a frame of an Atari game\n",
        "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
        "            batch_size: Integer, Number if transitions returned in a minibatch\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.frame_height = frame_height\n",
        "        self.frame_width = frame_width\n",
        "        self.agent_history_length = agent_history_length\n",
        "        self.batch_size = batch_size\n",
        "        self.count = 0\n",
        "        self.current = 0\n",
        "\n",
        "        # Pre-allocate memory\n",
        "        self.actions = np.empty(self.size, dtype=np.int32)\n",
        "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
        "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.terminal_flags = np.empty(self.size, dtype=bool)\n",
        "\n",
        "        # Pre-allocate memory for the states and new_states in a minibatch\n",
        "        self.states = np.empty((self.batch_size, self.agent_history_length,\n",
        "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.new_states = np.empty((self.batch_size, self.agent_history_length,\n",
        "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
        "\n",
        "    def add_experience(self, action, frame, reward, terminal):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action: An integer between 0 and env.action_space.n - 1\n",
        "                determining the action the agent performed  [a_t]\n",
        "            frame: A (84, 84, 1) frame of an Atari game in grayscale reached due to the action [s_t+1]\n",
        "            reward: A float determining the reward the agent received for performing an action [r_t]\n",
        "            terminal: A bool stating whether the episode terminated\n",
        "        \"\"\"\n",
        "        if frame.shape != (self.frame_height, self.frame_width):\n",
        "            raise ValueError('Dimension of frame is wrong!')\n",
        "        self.actions[self.current] = action\n",
        "        self.frames[self.current, ...] = frame\n",
        "        self.rewards[self.current] = reward\n",
        "        self.terminal_flags[self.current] = terminal\n",
        "        self.count = max(self.count, self.current + 1)\n",
        "        self.current = (self.current + 1) % self.size #overwrite replay buffer if necessary\n",
        "\n",
        "    def _get_state(self, index):\n",
        "        if self.count==0:\n",
        "            raise ValueError(\"The replay memory is empty!\")\n",
        "        if index < self.agent_history_length - 1:\n",
        "            raise ValueError(\"Index must be min 3\")\n",
        "        return self.frames[index - self.agent_history_length + 1:index + 1, ...]\n",
        "\n",
        "    def _get_valid_indices(self):\n",
        "        \"\"\"\n",
        "        We store all frames the agent sees in self.frames.\n",
        "        When a game terminates (terminal=True) at index i, frame at index i belongs\n",
        "        to a different episode than the frame at i+1. We want to avoid creating a state\n",
        "        with frames from two different episodes.\n",
        "        Finally we need to make sure that an index is not smaller than the number of\n",
        "        frames stacked toghether to create a state (self.agent_history_length=4),\n",
        "        so that a state and new_state can be sliced out of the array.\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            while True:\n",
        "                index = random.randint(self.agent_history_length, self.count - 1)\n",
        "                if index < self.agent_history_length:\n",
        "                    continue\n",
        "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
        "                    continue\n",
        "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
        "                    continue\n",
        "                break\n",
        "            self.indices[i] = index\n",
        "\n",
        "    def get_minibatch(self):\n",
        "        \"\"\"\n",
        "        Returns a minibatch of self.batch_size = 32 transitions\n",
        "        \"\"\"\n",
        "        if self.count < self.agent_history_length:\n",
        "            raise ValueError('Not enough memories to get a minibatch')\n",
        "\n",
        "        self._get_valid_indices()\n",
        "\n",
        "        for i, idx in enumerate(self.indices):\n",
        "            self.states[i] = self._get_state(idx - 1)\n",
        "            self.new_states[i] = self._get_state(idx)\n",
        "\n",
        "        minibatch = (np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[\n",
        "                     self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices])\n",
        "        return minibatch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "79kuK2wR9Ag8"
      },
      "source": [
        "## Pre-processing utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wK0O9cQq8-RR"
      },
      "outputs": [],
      "source": [
        "def pre_processing (frame):\n",
        "    # single Frame Processor from 210x160x3 to 84x84x1\n",
        "    frame_gray = np.dot(frame, [0.299, 0.587, 0.114])  # 210x160  convert gray scale\n",
        "    #plt.imshow(np.array(np.squeeze(frame_gray)), cmap='gray')\n",
        "    #plt.show()\n",
        "    frame_gray = frame_gray[31:195, 0:160]  # crop off upper score (31 lines) and below black area (15 lines)\n",
        "    #resized_img0 = Image.fromarray(frame_gray).resize(size=(84, 84), resample=Image.BILINEAR)  # 84x84x1\n",
        "    resized_img0 = Image.fromarray(frame_gray).resize(size=(84, 84), resample=Image.NEAREST)  # 84x84x1\n",
        "    #plt.imshow(np.array(np.squeeze(resized_img0)), cmap='gray')\n",
        "    #plt.show()\n",
        "    return asarray(resized_img0, dtype=np.uint8)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GhM_tWTr6GC_"
      },
      "source": [
        "## Main training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9fjrezO5frW"
      },
      "outputs": [],
      "source": [
        "print('main training loop')\n",
        "# Create models folder\n",
        "if not os.path.isdir('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "agent = DQNAgent()\n",
        "print(\"The environment has the following {} actions: {}\".format(env.action_space.n,\n",
        "                                                                env.unwrapped.get_action_meanings()))\n",
        "\n",
        "\n",
        "my_replay_memory = ReplayMemory(size=REPLAY_MEMORY_SIZE, batch_size=MINIBATCH_SIZE)   # (★)\n",
        "frame_number = 0   # total number of step conducted from the first match till the last\n",
        "# Iterate over episodes\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):  #1 episode = 1 match\n",
        "\n",
        "    # Restarting episode - reset episode reward and step number\n",
        "    episode_reward = 0\n",
        "    step = 1\n",
        "\n",
        "    # Reset environment and get initial state\n",
        "#    current_frame,_ = env.reset()   # single frame 84x84\n",
        "    #NEW VERSION : take care, the new format of the result is a tuple ...\n",
        "    current_frame,_ = env.reset()   # single frame 84x84\n",
        "\n",
        "    current_frame = pre_processing(current_frame)\n",
        "    current_state = np.dstack((current_frame, current_frame, current_frame, current_frame)) # create imm 84x84 grouping in 4 frames\n",
        "\n",
        "    # Reset flag and start iterating until episode ends\n",
        "    done = False\n",
        "    while not done:   #in the environment MountainCar-v0 done=False after 200 step\n",
        "                      #in the environment Breakout-v4 done=False after lost all lifes (num life =5)\n",
        "        # Exploration-exploitation trade-off\n",
        "        # Take new action\n",
        "        # train main network\n",
        "        # Set new state\n",
        "        # Add new reward\n",
        "\n",
        "        # Exploration-exploitation trade-off after a number of steps with completely random action\n",
        "        if np.random.random() > epsilon and frame_number > MIN_REPLAY_MEMORY_SIZE:\n",
        "            # Get action from DQN model: exploit\n",
        "            action = np.argmax(agent.get_qs(current_state))\n",
        "        else:\n",
        "            # Get random action: explore\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "\n",
        "        # Take new action\n",
        "#        new_frame, reward, done, _ = env.step(action)\n",
        "        #new format\n",
        "        new_frame, reward, done1,done2, _ = env.step(action)\n",
        "        done = done1 |done2\n",
        "        reward = np.clip(reward,-1,1)#clip reward to be between -1,1\n",
        "        # Set and preprocess new state\n",
        "        new_frame = pre_processing(new_frame)  # single frame 84x84 preprocessed\n",
        "        new_state = np.dstack((new_frame, current_state[:, :, 0], current_state[:, :, 1], current_state[:, :, 2]))# create imm 84x84 grouping in 4 frames\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if episode % SHOW_EVERY == 0: # and DEBUG: # plot one match every SHOW_EVERY\n",
        "            #time.sleep(0.01)\n",
        "            #print(f'Step: {step}')\n",
        "            env.render()\n",
        "\n",
        "        # Every step/frame we update replay memory with action, new frame, reward due to the action\n",
        "        my_replay_memory.add_experience(action, new_frame, reward, done)\n",
        "\n",
        "        # Every step we evaluate to train main network and/or to update weights of target network\n",
        "        if frame_number % UPDATE_MODEL == 0 and frame_number > MIN_REPLAY_MEMORY_SIZE:  # model update every 4 frame/action\n",
        "            # Get a minibatch of random samples from the replay memory\n",
        "            minibatch = my_replay_memory.get_minibatch()\n",
        "            agent.train(minibatch)                  #\n",
        "            if DEBUG:  # plot of the minibatch used to train (only 1 image over 4 into the frame)\n",
        "                fig = plt.figure(figsize=(8*4, 4*4))\n",
        "                for i in range(my_replay_memory.batch_size):\n",
        "                    plt.subplot(4, 8, i + 1)\n",
        "                    plt.imshow(minibatch[0][i, :, :, 0], cmap='jet')\n",
        "                plt.show()\n",
        "                plt.savefig('minibatch'+str(frame_number)+'.jpg')\n",
        "            del minibatch\n",
        "        if frame_number % UPDATE_TARGET_MODEL == 0 and frame_number > MIN_REPLAY_MEMORY_SIZE: # model target update every 10000 frame/action\n",
        "            agent.update_target_model()    # (9★)\n",
        "        #\n",
        "\n",
        "\n",
        "        # Set new state (9*)\n",
        "        current_state = new_state\n",
        "        step += 1\n",
        "        frame_number += 1\n",
        "\n",
        "    # Append episode reward to a list and log stats (every given number of episodes)\n",
        "    ep_rewards.append(episode_reward)\n",
        "    if episode % 10 == 0 and episode>AGGREGATE_STATS_EVERY:\n",
        "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        print(f'Episode:{episode:>5d}, frame_number:{frame_number:>7d}, '+\n",
        "              f'avg rew:{average_reward:>4.1f}, '+\n",
        "              f'max rew:{max(ep_rewards[-AGGREGATE_STATS_EVERY:]):>4.1f}, '+\n",
        "              f'min rew:{min(ep_rewards[-AGGREGATE_STATS_EVERY:]):>4.1f}, '+\n",
        "              f'current epsilon:{epsilon:>1.5f}')\n",
        "\n",
        "    # Decay epsilon. Only start after replay memory is over min size\n",
        "    if frame_number > MIN_REPLAY_MEMORY_SIZE:\n",
        "        if frame_number < REPLAY_MEMORY_SIZE:\n",
        "            epsilon = MID_EPSILON + EPSILON_DECAY_1*(REPLAY_MEMORY_SIZE-frame_number)\n",
        "        else:\n",
        "            epsilon = MIN_EPSILON +EPSILON_DECAY_2*(MAX_FRAMES-frame_number)\n",
        "    epsilon = np.clip(epsilon,MIN_EPSILON,MAX_EPSILON)\n",
        "\n",
        "    if episode % SAVE_EPISODE_EVERY == 0 and episode>AGGREGATE_STATS_EVERY:\n",
        "      agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}'+\n",
        "                       f'max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}'+\n",
        "                       f'min__{int(time.time())}.model')\n",
        "\n",
        "env.close()\n",
        "# Save model,\n",
        "agent.model.save(f'models/{MODEL_NAME}__'+\n",
        "                 f'{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_'+\n",
        "                 f'{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
        "\n",
        "plt.figure('stats')\n",
        "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n",
        "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"max rewards\")\n",
        "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"min rewards\")\n",
        "plt.legend(loc=2)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.savefig('grid.jpg')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AeiWxHec6eNu"
      },
      "source": [
        "# Debug and test the results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo4ng-NTHIW2"
      },
      "source": [
        "### Plot some mini-batch at random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahmcxc__C2cq",
        "outputId": "9bfe8bee-a357-4605-dbaf-905ffb1ceb3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "debug and test results\n"
          ]
        }
      ],
      "source": [
        "print('debug and test results')\n",
        "if(False):\n",
        "  print('plot some mini-batch at random')\n",
        "  minibatch = my_replay_memory.get_minibatch()\n",
        "  no_frames=my_replay_memory.agent_history_length\n",
        "  fig = plt.figure(figsize=(MINIBATCH_SIZE*4,no_frames*4))\n",
        "  for ii in range(no_frames):\n",
        "    for iii in range(MINIBATCH_SIZE):\n",
        "      plt.subplot(my_replay_memory.agent_history_length, MINIBATCH_SIZE, iii+ii*MINIBATCH_SIZE+1)\n",
        "      plt.imshow(minibatch[0][iii, :, :, ii], cmap='jet')#images are in minibatch[0] stored as minibatch[0].shape=(32, 84, 84, 4) ndarray\n",
        "      plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ohFv7I0j6aXy"
      },
      "outputs": [],
      "source": [
        "env_test = gym.make(\"ALE/Breakout-v5\",render_mode=\"rgb_array\")\n",
        "action_meanings=env_test.unwrapped.get_action_meanings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PMvAaiPN52S5"
      },
      "outputs": [],
      "source": [
        "if(False):\n",
        "  print('load model from directory model_bak')\n",
        "  loaded_model=keras.models.load_model('model_bak')\n",
        "  agent.model.set_weights(loaded_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WYR7QUIKoqx"
      },
      "outputs": [],
      "source": [
        "print('play game and draw results')\n",
        "import matplotlib.animation as animation\n",
        "images = []  # List to store the generated images\n",
        "\n",
        "\n",
        "current_frame,_ = env_test.reset()   # single frame 84x84\n",
        "current_frame_pre = pre_processing(current_frame)\n",
        "#initialize\n",
        "current_states=np.zeros(current_frame_pre.shape + (4,) )\n",
        "current_states[:,:,0]=current_frame_pre.copy()\n",
        "\n",
        "for jj in range(3):\n",
        "  new_frame, reward, done1,done2, _ = env_test.step(action)\n",
        "  current_states[:,:,jj+1]=pre_processing(new_frame)\n",
        "\n",
        "plt.figure('play')\n",
        "done=False\n",
        "count=0\n",
        "max_frames_play=20\n",
        "while (not done) and (count <max_frames_play):\n",
        "  count+=1\n",
        "  current_qs_list=agent.model.predict(current_states[None,...],verbose=0)# input_shape=[1,84, 84, 4]\n",
        "  best_action=np.argmax(current_qs_list)  \n",
        "  print('best action=',action_meanings[best_action],'frame=',str(count))\n",
        "  new_current_states=current_states.copy()\n",
        "  new_current_states[:,:,:-1]=current_states[:,:,1:]#shift old states down a unit, put new one after\n",
        "  new_frame, reward, done1,done2, _ = env_test.step(best_action)\n",
        "  new_current_states[:,:,-1]=pre_processing(new_frame)\n",
        "  done=done1|done2\n",
        "  images.append(new_frame)\n",
        "  if(done):\n",
        "    break\n",
        "#  plt.imshow(new_frame)\n",
        "#  plt.savefig(\"play_\"+str(count)+\".jpg\")\n",
        "#  plt.pause(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4W9BJag6M0r"
      },
      "outputs": [],
      "source": [
        "print('make animation')\n",
        "fig, ax = plt.subplots()\n",
        "# Function to update the plot for each frame of the animation\n",
        "def update(frame):\n",
        "    ax.imshow(images[frame])  # Display the corresponding image for the current frame\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update, frames=len(images), interval=400)\n",
        "ani.save('animation.gif',writer='pillow')  # Requires pillow to be installed\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMgznssL7Ps_"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image as Image_Ip\n",
        "from IPython.display import display\n",
        "display(Image_Ip(filename='animation.gif'))\n",
        "#if('ipykernel' in sys.modules):#we are in Jupyter Notebook\n",
        "#if('google.colab' in sys.modules):#we are in google colab"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
